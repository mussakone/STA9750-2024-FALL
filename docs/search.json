[
  {
    "objectID": "mp03.html#task-1-download-congressional-shapefiles-1976-2012",
    "href": "mp03.html#task-1-download-congressional-shapefiles-1976-2012",
    "title": "This Is My Mini Project #3: Electoral College System and Presidential Elections",
    "section": "Task 1: Download Congressional Shapefiles 1976-2012",
    "text": "Task 1: Download Congressional Shapefiles 1976-2012\nIn this task, we aim to automate the process of downloading U.S. congressional district shapefiles for each Congress from 1976 to 2012. These shapefiles, provided by Lewis et al., allow us to visualize and analyze geographic boundaries that define congressional districts over time.\n\n\nClick to Show/Hide Code\n# Load required libraries\nlibrary(httr)    # For HTTP requests\nlibrary(glue)    # For formatted strings\nlibrary(fs)      # For file and directory handling\n\n# Set up directory for shapefiles\ndownload_dir &lt;- \"data/shapefiles\"\nif (!dir_exists(download_dir)) dir_create(download_dir)\n\n# Define base URL and range of Congress sessions\nbase_url &lt;- \"https://cdmaps.polisci.ucla.edu/shp/districts\"\ncongress_years &lt;- seq(94, 112)  # Congresses from 1976 to 2012 (94th to 112th)\n\n# Loop through Congress sessions to download shapefiles\nfor (congress in congress_years) {\n  # Define file name and path\n  zip_file_name &lt;- glue(\"c{congress}_dist.zip\")\n  zip_file_path &lt;- path(download_dir, zip_file_name)\n  \n  # Download only if the file does not already exist\n  if (!file_exists(zip_file_path)) {\n    # Construct the download URL\n    download_url &lt;- glue(\"{base_url}/{zip_file_name}\")\n    \n    # Try downloading the file\n    cat(\"Attempting to download:\", download_url, \"\\n\")\n    response &lt;- tryCatch({\n      GET(download_url, write_disk(zip_file_path, overwrite = TRUE))\n    }, error = function(e) {\n      cat(\"Error downloading:\", zip_file_name, \"\\n\")\n      NULL\n    })\n    \n    # Check if the download was successful\n    if (!is.null(response) && status_code(response) == 200) {\n      cat(\"Downloaded:\", zip_file_name, \"\\n\")\n      \n      # Extract .shp files from the zip file if download successful\n      extracted_dir &lt;- path(download_dir, glue(\"congress_{congress}\"))\n      if (!dir_exists(extracted_dir)) dir_create(extracted_dir)\n      \n      # Unzip only the .shp files\n      shp_files &lt;- grep(\"\\\\.shp$\", unzip(zip_file_path, list = TRUE)$Name, value = TRUE)\n      unzip(zip_file_path, files = shp_files, exdir = extracted_dir)\n      cat(\"Extracted shapefiles for Congress\", congress, \"\\n\")\n    }\n  } else {\n    cat(\"File already exists:\", zip_file_name, \"- Skipping download.\\n\")\n  }\n}\n\n\nFile already exists: c94_dist.zip - Skipping download.\nFile already exists: c95_dist.zip - Skipping download.\nFile already exists: c96_dist.zip - Skipping download.\nFile already exists: c97_dist.zip - Skipping download.\nFile already exists: c98_dist.zip - Skipping download.\nFile already exists: c99_dist.zip - Skipping download.\nFile already exists: c100_dist.zip - Skipping download.\nFile already exists: c101_dist.zip - Skipping download.\nFile already exists: c102_dist.zip - Skipping download.\nFile already exists: c103_dist.zip - Skipping download.\nFile already exists: c104_dist.zip - Skipping download.\nFile already exists: c105_dist.zip - Skipping download.\nFile already exists: c106_dist.zip - Skipping download.\nFile already exists: c107_dist.zip - Skipping download.\nFile already exists: c108_dist.zip - Skipping download.\nFile already exists: c109_dist.zip - Skipping download.\nFile already exists: c110_dist.zip - Skipping download.\nFile already exists: c111_dist.zip - Skipping download.\nFile already exists: c112_dist.zip - Skipping download.\n\n\nClick to Show/Hide Code\ncat(\"All shapefiles processed successfully.\\n\")\n\n\nAll shapefiles processed successfully."
  },
  {
    "objectID": "mp03.html#task-2-automated-download-of-congressional-shapefiles-2014-2022",
    "href": "mp03.html#task-2-automated-download-of-congressional-shapefiles-2014-2022",
    "title": "This Is My Mini Project #3: Electoral College System and Presidential Elections",
    "section": "Task 2: Automated Download of Congressional Shapefiles (2014-2022)",
    "text": "Task 2: Automated Download of Congressional Shapefiles (2014-2022)\nIn this task, we aim to automate the process of downloading congressional district shapefiles for each U.S. Congress from 2014 to 2022. These shapefiles, available from the U.S. Census Bureau, provide geographical boundary data for congressional districts, which is crucial for spatial analysis and mapping of political and demographic trends. Additionally, we will extract only the necessary .shp files from each downloaded zip archive, organizing them with a clear naming convention for easy reference in future analysis. This structured approach will facilitate accurate and reproducible results for any subsequent spatial analyses or visualizations involving U.S. congressional districts.\n\n\nClick to Show/Hide Code\n# Load required libraries\nlibrary(httr)    # For HTTP requests\nlibrary(glue)    # For formatted strings\nlibrary(fs)      # For file and directory handling\nlibrary(xml2)    # For parsing HTML content\nlibrary(rvest)   # For web scraping\n\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\n\nClick to Show/Hide Code\n# Define base URL and create download directory\nbase_url &lt;- \"https://www2.census.gov/geo/tiger/TIGER{year}/CD/\"\ndownload_dir &lt;- \"data/congress_shapefiles\"\nif (!dir_exists(download_dir)) dir_create(download_dir)\n\n# Define the years corresponding to each Congress session (2014-2022)\nyears &lt;- seq(2014, 2022, 2)\n\n# Function to download and extract shapefiles for each year\nfor (year in years) {\n  year_url &lt;- glue(base_url, year = year)\n  \n  # Get list of zip files available for each year\n  page &lt;- tryCatch({\n    read_html(year_url)\n  }, error = function(e) {\n    cat(\"Could not access:\", year_url, \"\\n\")\n    return(NULL)\n  })\n  \n  # If the page was successfully accessed, proceed\n  if (!is.null(page)) {\n    # Get links to all zip files in the directory\n    zip_files &lt;- page %&gt;% html_nodes(\"a\") %&gt;% html_attr(\"href\") %&gt;% \n                 .[grepl(\"\\\\.zip$\", .)]\n    \n    # Download each zip file\n    for (zip_file in zip_files) {\n      file_name &lt;- glue(\"congress_{year}_{zip_file}\")\n      file_path &lt;- path(download_dir, file_name)\n      file_url &lt;- paste0(year_url, zip_file)\n      \n      # Download only if the file does not already exist\n      if (!file_exists(file_path)) {\n        cat(\"Downloading:\", file_url, \"\\n\")\n        GET(file_url, write_disk(file_path, overwrite = TRUE))\n        \n        # Extract the .shp file from the zip\n        extracted_dir &lt;- path(download_dir, glue(\"congress_{year}\"))\n        if (!dir_exists(extracted_dir)) dir_create(extracted_dir)\n        \n        unzip(file_path, exdir = extracted_dir)\n        cat(\"Extracted shapefiles for Congress session:\", year, \"\\n\")\n      } else {\n        cat(\"File already exists:\", file_name, \"- Skipping download.\\n\")\n      }\n    }\n  }\n}\n\n\nFile already exists: congress_2014_tl_2014_us_cd114.zip - Skipping download.\nFile already exists: congress_2016_tl_2016_us_cd115.zip - Skipping download.\nFile already exists: congress_2018_tl_2018_us_cd116.zip - Skipping download.\nFile already exists: congress_2020_tl_2020_us_cd116.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_01_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_02_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_04_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_05_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_06_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_08_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_09_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_10_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_11_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_12_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_13_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_15_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_16_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_17_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_18_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_19_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_20_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_21_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_22_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_23_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_24_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_25_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_26_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_27_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_28_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_29_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_30_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_31_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_32_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_33_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_34_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_35_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_36_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_37_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_38_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_39_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_40_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_41_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_42_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_44_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_45_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_46_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_47_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_48_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_49_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_50_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_51_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_53_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_54_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_55_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_56_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_60_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_66_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_69_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_72_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_78_cd118.zip - Skipping download.\nFile already exists: congress_2022_tl_2022_us_cd116.zip - Skipping download.\n\n\nClick to Show/Hide Code\ncat(\"All congressional shapefiles processed successfully.\\n\")\n\n\nAll congressional shapefiles processed successfully."
  },
  {
    "objectID": "mp03.html#task-3-exploration-of-vote-count-data",
    "href": "mp03.html#task-3-exploration-of-vote-count-data",
    "title": "This Is My Mini Project #3: Electoral College System and Presidential Elections",
    "section": "Task 3: Exploration Of Vote Count Data",
    "text": "Task 3: Exploration Of Vote Count Data\nThis task will require to go back to data we intially downloaded at the start of this report to answer several questions regarding the House of Representatives, the fusion system, and presidential candidate trends in different states."
  },
  {
    "objectID": "mp03.html#which-states-have-gained-and-lost-the-most-seats-in-the-us-house-of-representatives-between-1976-and-2022",
    "href": "mp03.html#which-states-have-gained-and-lost-the-most-seats-in-the-us-house-of-representatives-between-1976-and-2022",
    "title": "This Is My Mini Project #3: Electoral College System and Presidential Elections",
    "section": "1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?",
    "text": "1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\n\nClick to Show/Hide Code\n# Load necessary libraries\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nClick to Show/Hide Code\nlibrary(readr)\nlibrary(knitr)\n\n# Load the data\n  house_data &lt;- read_csv(file.path(data_dir, \"1976-2022-house.csv\"))\n\n\nRows: 32452 Columns: 20\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): state, state_po, office, stage, candidate, party, mode\ndbl (8): year, state_fips, state_cen, state_ic, district, candidatevotes, to...\nlgl (5): runoff, special, writein, unofficial, fusion_ticket\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nClick to Show/Hide Code\n# Count unique districts per state and year\ndistrict_counts &lt;- house_data %&gt;%\n  filter(!is.na(district)) %&gt;%\n  group_by(year, state) %&gt;%\n  summarize(num_districts = n_distinct(district), .groups = 'drop')\n\n# Get counts for 1976 and 2022\ndistrict_counts_1976 &lt;- district_counts %&gt;% filter(year == 1976) %&gt;% select(state, num_districts)\ndistrict_counts_2022 &lt;- district_counts %&gt;% filter(year == 2022) %&gt;% select(state, num_districts)\n\n# Calculate change in seats from 1976 to 2022\nseat_changes &lt;- district_counts_2022 %&gt;%\n  rename(num_districts_2022 = num_districts) %&gt;%\n  inner_join(district_counts_1976, by = \"state\", suffix = c(\"_2022\", \"_1976\")) %&gt;%\n  mutate(seat_change = num_districts_2022 - num_districts) %&gt;%\n  arrange(desc(seat_change))\n\n# Top 5 states with most gain and most loss in seats\nseat_changes_top_gain_loss &lt;- seat_changes %&gt;%\n  slice(c(1:5, (n()-4):n()))\n\n# Display the result as a formatted table\nkable(seat_changes_top_gain_loss, caption = \"Top 5 States with Most Gains and Losses in House Seats (1976-2022)\")\n\n\n\nTop 5 States with Most Gains and Losses in House Seats (1976-2022)\n\n\nstate\nnum_districts_2022\nnum_districts\nseat_change\n\n\n\n\nTEXAS\n38\n24\n14\n\n\nFLORIDA\n28\n15\n13\n\n\nCALIFORNIA\n52\n43\n9\n\n\nARIZONA\n9\n4\n5\n\n\nGEORGIA\n14\n10\n4\n\n\nMICHIGAN\n13\n19\n-6\n\n\nILLINOIS\n17\n24\n-7\n\n\nOHIO\n15\n23\n-8\n\n\nPENNSYLVANIA\n17\n25\n-8\n\n\nNEW YORK\n26\n39\n-13"
  },
  {
    "objectID": "mp03.html#new-york-state-has-a-unique-fusion-voting-system-where-one-candidate-can-appear-on-multiple-lines-on-the-ballot-and-their-vote-counts-are-totaled.-for-instance-in-2022-jerrold-nadler-appeared-on-both-the-democrat-and-working-families-party-lines-for-nys-12th-congressional-district.-he-received-200890-votes-total-184872-as-a-democrat-and-16018-as-wfp-easily-defeating-michael-zumbluskas-who-received-44173-votes-across-three-party-lines-republican-conservative-and-parent.are-there-any-elections-in-our-data-where-the-election-would-have-had-a-different-outcome-if-the-fusion-system-was-not-used-and-candidates-only-received-the-votes-their-received-from-their-major-party-line-democrat-or-republican-and-not-their-total-number-of-votes-across-all-lines",
    "href": "mp03.html#new-york-state-has-a-unique-fusion-voting-system-where-one-candidate-can-appear-on-multiple-lines-on-the-ballot-and-their-vote-counts-are-totaled.-for-instance-in-2022-jerrold-nadler-appeared-on-both-the-democrat-and-working-families-party-lines-for-nys-12th-congressional-district.-he-received-200890-votes-total-184872-as-a-democrat-and-16018-as-wfp-easily-defeating-michael-zumbluskas-who-received-44173-votes-across-three-party-lines-republican-conservative-and-parent.are-there-any-elections-in-our-data-where-the-election-would-have-had-a-different-outcome-if-the-fusion-system-was-not-used-and-candidates-only-received-the-votes-their-received-from-their-major-party-line-democrat-or-republican-and-not-their-total-number-of-votes-across-all-lines",
    "title": "This Is My Mini Project #3: Electoral College System and Presidential Elections",
    "section": "2 New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent).Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?",
    "text": "2 New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent).Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\n\n\nClick to Show/Hide Code\n# Load required libraries\nlibrary(dplyr)\nlibrary(readr)\nlibrary(knitr)\n\n# Load the data\nhouse_data &lt;- read_csv(file.path(data_dir, \"1976-2022-house.csv\"))\n\n\nRows: 32452 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): state, state_po, office, stage, candidate, party, mode\ndbl (8): year, state_fips, state_cen, state_ic, district, candidatevotes, to...\nlgl (5): runoff, special, writein, unofficial, fusion_ticket\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nClick to Show/Hide Code\n# Identify major party lines (Democrat and Republican) for all states\nhouse_data &lt;- house_data %&gt;%\n  mutate(is_major_party = ifelse(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\"), TRUE, FALSE))\n\n# Aggregate votes by candidate across all lines and calculate total and major-only votes\ntotal_votes_by_candidate &lt;- house_data %&gt;%\n  group_by(year, state, district, candidate, party) %&gt;%\n  summarize(total_votes = sum(candidatevotes, na.rm = TRUE),\n            major_party_votes = sum(candidatevotes[is_major_party], na.rm = TRUE),\n            .groups = 'drop')\n\n# Check if aggregation resulted in any data\nif (nrow(total_votes_by_candidate) == 0) {\n  stop(\"No aggregated vote data available for the specified candidates.\")\n}\n\n# Identify potential outcome changes across all states\noutcome_changes &lt;- total_votes_by_candidate %&gt;%\n  group_by(year, state, district) %&gt;%\n  arrange(desc(total_votes)) %&gt;%\n  mutate(winner_total = candidate[which.max(total_votes)],\n         winner_major_only = candidate[which.max(major_party_votes)],\n         outcome_change = winner_total != winner_major_only) %&gt;%\n  ungroup() %&gt;%\n  select(year, state, district, candidate, party, total_votes, major_party_votes, winner_total, winner_major_only, outcome_change)\n\n# Sub-sample the result for a more manageable display size \noutcome_changes_sample &lt;- outcome_changes %&gt;% sample_n(50)\n\n# Display the result as a table with all necessary details\nkable(outcome_changes_sample, caption = \"Comparison of Presidential and Congressional Votes by State, Year, and Party (including outcome changes)\")\n\n\n\nComparison of Presidential and Congressional Votes by State, Year, and Party (including outcome changes)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nstate\ndistrict\ncandidate\nparty\ntotal_votes\nmajor_party_votes\nwinner_total\nwinner_major_only\noutcome_change\n\n\n\n\n2010\nVIRGINIA\n11\nGERALD E CONNOLLY\nDEMOCRAT\n111720\n111720\nGERALD E CONNOLLY\nGERALD E CONNOLLY\nFALSE\n\n\n2000\nKENTUCKY\n1\nED WHITFIELD\nREPUBLICAN\n132115\n132115\nED WHITFIELD\nED WHITFIELD\nFALSE\n\n\n2022\nMASSACHUSETTS\n2\nJAMES P MCGOVERN\nDEMOCRAT\n180639\n180639\nJAMES P MCGOVERN\nJAMES P MCGOVERN\nFALSE\n\n\n2014\nNEW YORK\n21\nELISE M STEFANIK\nREPUBLICAN\n79615\n79615\nELISE M STEFANIK\nELISE M STEFANIK\nFALSE\n\n\n2016\nNEW YORK\n21\nELISE M STEFANIK\nCONSERVATIVE\n15526\n0\nELISE M STEFANIK\nELISE M STEFANIK\nFALSE\n\n\n2022\nNORTH CAROLINA\n4\nCOURTNEY GEELS\nREPUBLICAN\n96442\n96442\nVALERIE P FOUSHEE\nVALERIE P FOUSHEE\nFALSE\n\n\n1990\nCALIFORNIA\n5\nNANCY PELOSI\nDEMOCRAT\n120633\n120633\nNANCY PELOSI\nNANCY PELOSI\nFALSE\n\n\n1990\nRHODE ISLAND\n1\nRONALD K MACHTLEY\nREPUBLICAN\n89963\n89963\nRONALD K MACHTLEY\nRONALD K MACHTLEY\nFALSE\n\n\n1990\nIDAHO\n1\nLARRY LAROCCO\nDEMOCRAT\n85054\n85054\nLARRY LAROCCO\nLARRY LAROCCO\nFALSE\n\n\n2010\nWISCONSIN\n1\nBLANK VOTE/SCATTERING\nNA\n134\n0\nPAUL RYAN\nPAUL RYAN\nFALSE\n\n\n1992\nKENTUCKY\n4\nFLOYD G POORE\nDEMOCRAT\n86890\n86890\nJIM BUNNING\nJIM BUNNING\nFALSE\n\n\n1978\nOHIO\n12\nJAMES L BAUMANN\nDEMOCRAT\n61698\n61698\nSAMUEL L DEVINE\nSAMUEL L DEVINE\nFALSE\n\n\n1994\nTEXAS\n24\nED HARRISON\nREPUBLICAN\n58062\n58062\nMARTIN FROST\nMARTIN FROST\nFALSE\n\n\n1994\nPENNSYLVANIA\n8\nJAY RUSSELL\nLIBERTARIAN\n7925\n0\nJAMES C GREENWOOD\nJAMES C GREENWOOD\nFALSE\n\n\n1982\nCALIFORNIA\n5\nJUSTIN RAIMONDO\nLIBERTARIAN\n2904\n0\nPHILLIP BURTON\nPHILLIP BURTON\nFALSE\n\n\n2004\nALABAMA\n5\nGERALD “GERRY” WALLACE\nREPUBLICAN\n74145\n74145\nROBERT E “BUD” CRAMER JR\nROBERT E “BUD” CRAMER JR\nFALSE\n\n\n2022\nNEW MEXICO\n2\nYVETTE HERRELL\nREPUBLICAN\n95636\n95636\nGABRIEL VASQUEZ\nGABRIEL VASQUEZ\nFALSE\n\n\n2018\nWYOMING\n0\nRICHARD BRUBAKER\nLIBERTARIAN\n6918\n0\nLIZ CHENEY\nLIZ CHENEY\nFALSE\n\n\n1984\nARIZONA\n1\nHARRY W BRAUN III\nDEMOCRAT\n45609\n45609\nJOHN MCCAIN\nJOHN MCCAIN\nFALSE\n\n\n2000\nWEST VIRGINIA\n3\nNICK J RAHALL II\nDEMOCRAT\n146807\n146807\nNICK J RAHALL II\nNICK J RAHALL II\nFALSE\n\n\n2022\nTEXAS\n37\nCLARK PATTERSON\nLIBERTARIAN\n6332\n0\nLLOYD DOGGETT\nLLOYD DOGGETT\nFALSE\n\n\n1998\nILLINOIS\n2\nMATTHEW JOSEPH BEAUCHAMP\nLIBERTARIAN\n1608\n0\nJESSE L JACKSON JR\nJESSE L JACKSON JR\nFALSE\n\n\n2006\nNEW YORK\n4\nMARTIN W BLESSINGER\nREPUBLICAN\n48121\n48121\nCAROLYN MCCARTHY\nCAROLYN MCCARTHY\nFALSE\n\n\n2012\nTEXAS\n9\nVANESSA FOSTER\nGREEN\n1743\n0\nAL GREEN\nAL GREEN\nFALSE\n\n\n1992\nTENNESSEE\n8\nDAVID L WARD\nINDEPENDENT\n6930\n0\nJOHN S TANNER\nJOHN S TANNER\nFALSE\n\n\n1998\nNEW YORK\n22\nFRANCIS A GIROUX\nRIGHT TO LIFE\n5051\n0\nJOHN E SWEENY\nJOHN E SWEENY\nFALSE\n\n\n1980\nCALIFORNIA\n12\nKIRSTEN OLSEN\nDEMOCRAT\n37009\n37009\nPAUL N “PETE” MCCLOSKEY JR\nPAUL N “PETE” MCCLOSKEY JR\nFALSE\n\n\n2000\nNEW JERSEY\n7\nSHAWN GIANELLA\nINDEPENDENT\n386\n0\nMIKE FERGUSON\nMIKE FERGUSON\nFALSE\n\n\n1998\nNEW YORK\n13\nANITA LERMAN\nINDEPENDENCE\n1245\n0\nVITO FOSSELLA\nVITO FOSSELLA\nFALSE\n\n\n2000\nMINNESOTA\n6\nBILL LUTHER\nDEMOCRATIC-FARMER-LABOR\n176340\n0\nBILL LUTHER\nJOHN KLINE\nTRUE\n\n\n1996\nCALIFORNIA\n10\nJOHN PLACE\nREFORM\n6354\n0\nELLEN O TAUSCHER\nELLEN O TAUSCHER\nFALSE\n\n\n2000\nNEW YORK\n8\nJERROLD NADLER\nWORKING FAMILIES\n6742\n0\nJERROLD NADLER\nJERROLD NADLER\nFALSE\n\n\n2020\nCALIFORNIA\n26\nRONDA BALDWIN-KENNEDY\nREPUBLICAN\n135877\n135877\nJULIA BROWNLEY\nJULIA BROWNLEY\nFALSE\n\n\n1982\nILLINOIS\n3\nRICHARD D MURPHY\nREPUBLICAN\n48268\n48268\nMARTIN A RUSSO\nMARTIN A RUSSO\nFALSE\n\n\n1992\nNEW JERSEY\n1\nROBERT E ANDREWS\nDEMOCRAT\n153525\n153525\nROBERT E ANDREWS\nROBERT E ANDREWS\nFALSE\n\n\n2012\nMASSACHUSETTS\n6\nJOHN F TIERNEY\nDEMOCRAT\n180942\n180942\nJOHN F TIERNEY\nJOHN F TIERNEY\nFALSE\n\n\n1994\nTEXAS\n25\nKEN BENTSEN\nDEMOCRAT\n61959\n61959\nKEN BENTSEN\nKEN BENTSEN\nFALSE\n\n\n1984\nTEXAS\n18\nMICKEY LELAND\nDEMOCRAT\n109626\n109626\nMICKEY LELAND\nMICKEY LELAND\nFALSE\n\n\n2012\nMISSISSIPPI\n2\nBILL MARCY\nREPUBLICAN\n99160\n99160\nBENNIE G THOMPSON\nBENNIE G THOMPSON\nFALSE\n\n\n2010\nMICHIGAN\n1\nGLENN A WILSON\nNO PARTY AFFILIATION\n7847\n0\nDAN BENISHEK\nDAN BENISHEK\nFALSE\n\n\n1976\nPENNSYLVANIA\n12\nTHEODORE L HUMES\nREPUBLICAN\n58489\n58489\nJOHN P MURTHA\nJOHN P MURTHA\nFALSE\n\n\n1978\nCALIFORNIA\n4\nVIC FAZIO\nDEMOCRAT\n87764\n87764\nVIC FAZIO\nVIC FAZIO\nFALSE\n\n\n2004\nFLORIDA\n22\nJACK MCLAIN\nCONSTITUTION PARTY OF FLORIDA\n5260\n0\nE CLAY SHAW JR\nE CLAY SHAW JR\nFALSE\n\n\n1990\nMISSOURI\n4\nIKE SKELTON\nDEMOCRAT\n105527\n105527\nIKE SKELTON\nIKE SKELTON\nFALSE\n\n\n2004\nSOUTH CAROLINA\n2\nSTEVE LEFEMINE\nCONSTITUTION\n4447\n0\nJOE WILSON\nJOE WILSON\nFALSE\n\n\n2016\nOHIO\n7\nBOB GIBBS\nREPUBLICAN\n198221\n198221\nBOB GIBBS\nBOB GIBBS\nFALSE\n\n\n2012\nNORTH CAROLINA\n3\nERIK ANDERSON\nDEMOCRAT\n114314\n114314\nWALTER B JONES\nWALTER B JONES\nFALSE\n\n\n1996\nOHIO\n16\nRALPH REGULA\nREPUBLICAN\n159314\n159314\nRALPH REGULA\nRALPH REGULA\nFALSE\n\n\n1996\nALABAMA\n3\nBOB RILEY\nREPUBLICAN\n98353\n98353\nBOB RILEY\nBOB RILEY\nFALSE\n\n\n2016\nNEW YORK\n18\nVOID VOTE\nNA\n45\n0\nSEAN PATRICK MALONEY\nSEAN PATRICK MALONEY\nFALSE\n\n\n\n\n\n##3 Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state?\nDoes this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\n\nClick to Show/Hide Code\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(readr)\nlibrary(knitr)\n\n# Load the data files - update paths if needed\nhouse_data &lt;- read_csv(file.path(data_dir, \"1976-2022-house.csv\"))\n\n\nRows: 32452 Columns: 20\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): state, state_po, office, stage, candidate, party, mode\ndbl (8): year, state_fips, state_cen, state_ic, district, candidatevotes, to...\nlgl (5): runoff, special, writein, unofficial, fusion_ticket\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nClick to Show/Hide Code\npresident_data &lt;- read_csv(file.path(data_dir, \"1976-2020-president.csv\"))\n\n\nRows: 4287 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): state, state_po, office, candidate, party_detailed, party_simplified\ndbl (7): year, state_fips, state_cen, state_ic, candidatevotes, totalvotes, ...\nlgl (2): writein, notes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nClick to Show/Hide Code\n# Create a new `party_detailed` column in `house_data` based on `party`\nhouse_data &lt;- house_data %&gt;%\n  mutate(party_detailed = party)  # Copy `party` values to a new `party_detailed` column\n\n# Aggregate votes by year, state, and party_detailed for congressional data\nhouse_data &lt;- house_data %&gt;%\n  group_by(year, state, party_detailed) %&gt;%\n  summarize(total_congress_votes = sum(candidatevotes, na.rm = TRUE), .groups = 'drop')\n\n# Aggregate votes by year, state, and party_detailed for presidential data\npresident_data &lt;- president_data %&gt;%\n  group_by(year, state, party_detailed) %&gt;%\n  summarize(total_presidential_votes = sum(candidatevotes, na.rm = TRUE), .groups = 'drop')\n\n# Join datasets by year, state, and party_detailed\ncombined_data &lt;- house_data %&gt;%\n  inner_join(president_data, by = c(\"year\", \"state\", \"party_detailed\")) %&gt;%\n  mutate(vote_difference = total_presidential_votes - total_congress_votes)\n\n# Select relevant columns for the table and arrange by year and state\ntable_data &lt;- combined_data %&gt;%\n  select(year, state, party_detailed, total_presidential_votes, total_congress_votes, vote_difference) %&gt;%\n  arrange(year, state)\n\n# Sub-sample the result for a more manageable display size \ntable_data_sample &lt;- table_data %&gt;% sample_n(50)\n\n# Display the result as a formatted table\nkable(table_data_sample, caption = \"Comparison of Presidential and Congressional Votes by State, Year, and Party (Detailed)\")\n\n\n\nComparison of Presidential and Congressional Votes by State, Year, and Party (Detailed)\n\n\n\n\n\n\n\n\n\n\nyear\nstate\nparty_detailed\ntotal_presidential_votes\ntotal_congress_votes\nvote_difference\n\n\n\n\n1984\nOREGON\nNA\n4348\n198\n4150\n\n\n1980\nIOWA\nLIBERTARIAN\n13123\n2359\n10764\n\n\n1988\nKANSAS\nDEMOCRAT\n422636\n366757\n55879\n\n\n2012\nSOUTH CAROLINA\nDEMOCRAT\n865941\n714191\n151750\n\n\n1992\nPENNSYLVANIA\nDEMOCRAT\n2239164\n2154372\n84792\n\n\n1976\nWISCONSIN\nREPUBLICAN\n1004987\n760740\n244247\n\n\n1984\nILLINOIS\nSOCIALIST WORKERS\n2132\n8086\n-5954\n\n\n2004\nKENTUCKY\nINDEPENDENT\n8856\n5069\n3787\n\n\n2008\nALABAMA\nNA\n3705\n15998\n-12293\n\n\n2016\nNEW JERSEY\nDEMOCRAT\n2148278\n1821620\n326658\n\n\n2000\nMARYLAND\nREPUBLICAN\n813827\n856306\n-42479\n\n\n1988\nVERMONT\nINDEPENDENT\n275\n90026\n-89751\n\n\n1988\nMARYLAND\nREPUBLICAN\n876167\n624021\n252146\n\n\n2004\nNEW JERSEY\nINDEPENDENT\n30258\n48419\n-18161\n\n\n1996\nOHIO\nDEMOCRAT\n2148222\n2033028\n115194\n\n\n2000\nVERMONT\nREPUBLICAN\n119775\n51977\n67798\n\n\n1980\nARIZONA\nSOCIALIST WORKERS\n1100\n2043\n-943\n\n\n1980\nMASSACHUSETTS\nCITIZENS\n2056\n3290\n-1234\n\n\n1996\nNEW YORK\nFREEDOM\n11393\n39379\n-27986\n\n\n2008\nARIZONA\nINDEPENDENT\n11301\n9394\n1907\n\n\n2012\nMAINE\nREPUBLICAN\n292276\n265982\n26294\n\n\n2020\nINDIANA\nREPUBLICAN\n1729519\n1738745\n-9226\n\n\n1976\nNORTH DAKOTA\nAMERICAN\n3698\n4600\n-902\n\n\n1976\nNEW JERSEY\nSOCIALIST WORKERS\n1184\n330\n854\n\n\n1992\nWISCONSIN\nINDEPENDENT\n551997\n17689\n534308\n\n\n1980\nMINNESOTA\nDEMOCRAT\n954173\n905793\n48380\n\n\n2004\nUTAH\nLIBERTARIAN\n3375\n3691\n-316\n\n\n1992\nVIRGINIA\nINDEPENDENT\n363768\n69068\n294700\n\n\n1996\nMISSISSIPPI\nLIBERTARIAN\n2809\n9429\n-6620\n\n\n1984\nWASHINGTON\nPOPULIST\n5724\n6855\n-1131\n\n\n2008\nOHIO\nDEMOCRAT\n2940044\n2752111\n187933\n\n\n1996\nCALIFORNIA\nNATURAL LAW\n15403\n126822\n-111419\n\n\n2004\nWEST VIRGINIA\nDEMOCRAT\n326541\n415396\n-88855\n\n\n2020\nMISSISSIPPI\nREPUBLICAN\n756764\n806832\n-50068\n\n\n2020\nWISCONSIN\nDEMOCRAT\n1630866\n1566671\n64195\n\n\n1996\nSOUTH DAKOTA\nINDEPENDENT\n32478\n17263\n15215\n\n\n2004\nMASSACHUSETTS\nNA\n22095\n357800\n-335705\n\n\n1996\nNEVADA\nNATURAL LAW\n545\n7755\n-7210\n\n\n1988\nRHODE ISLAND\nDEMOCRAT\n225123\n140270\n84853\n\n\n1988\nWEST VIRGINIA\nDEMOCRAT\n341016\n436616\n-95600\n\n\n2004\nTEXAS\nNA\n12341\n13961\n-1620\n\n\n1996\nFLORIDA\nDEMOCRAT\n2546870\n2036620\n510250\n\n\n2004\nALASKA\nGREEN\n1058\n11434\n-10376\n\n\n2004\nGEORGIA\nNA\n3085\n77\n3008\n\n\n1980\nNEW JERSEY\nSOCIALIST WORKERS\n2198\n1869\n329\n\n\n2020\nINDIANA\nDEMOCRAT\n1242416\n1194901\n47515\n\n\n1988\nTEXAS\nDEMOCRAT\n2352748\n2735940\n-383192\n\n\n1988\nMISSOURI\nREPUBLICAN\n1084953\n909599\n175354\n\n\n2016\nVIRGINIA\nINDEPENDENT\n54054\n36580\n17474\n\n\n2008\nNORTH CAROLINA\nLIBERTARIAN\n25722\n19605\n6117\n\n\n\n\n\n\n\nClick to Show/Hide Code\nlibrary(ggplot2)\nlibrary(sf)\n\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\nClick to Show/Hide Code\nif(!file.exists(\"nyc_borough_boundaries.zip\")){\n    download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\", \n              destfile=\"nyc_borough_boundaries.zip\")\n}\n\n##-\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"nyc_borough_boundaries.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_sf &lt;- read_sf(fname_shp)\nnyc_sf\n\n\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49613 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS84(DD)\n# A tibble: 5 × 5\n  boro_code boro_name      shape_area shape_leng                        geometry\n      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;              &lt;MULTIPOLYGON [°]&gt;\n1         3 Brooklyn      1934142776.    728147. (((-73.86327 40.58388, -73.863…\n2         5 Staten Island 1623618684.    325910. (((-74.05051 40.56642, -74.050…\n3         1 Manhattan      636646082.    360038. (((-74.01093 40.68449, -74.011…\n4         2 Bronx         1187174772.    463181. (((-73.89681 40.79581, -73.896…\n5         4 Queens        3041418004.    888197. (((-73.82645 40.59053, -73.826…"
  },
  {
    "objectID": "mp03.html#task-4-automate-zip-file-extraction-in-this-task-we-are-creating-a-reusable-function-read_shp_from_zip-to-efficiently-handle-shapefile-data-stored-within-zip-archives-above-.-rather-than-manually-unzipping-files-and-locating-shapefiles-this-function-automates-the-process-by-locating-extracting-and-seamlessly-loading.",
    "href": "mp03.html#task-4-automate-zip-file-extraction-in-this-task-we-are-creating-a-reusable-function-read_shp_from_zip-to-efficiently-handle-shapefile-data-stored-within-zip-archives-above-.-rather-than-manually-unzipping-files-and-locating-shapefiles-this-function-automates-the-process-by-locating-extracting-and-seamlessly-loading.",
    "title": "This Is My Mini Project #3: Electoral College System and Presidential Elections",
    "section": "Task 4: Automate Zip File Extraction In this task, we are creating a reusable function, read_shp_from_zip(), to efficiently handle shapefile data stored within zip archives above . Rather than manually unzipping files and locating shapefiles, this function automates the process by locating, extracting and seamlessly loading.",
    "text": "Task 4: Automate Zip File Extraction In this task, we are creating a reusable function, read_shp_from_zip(), to efficiently handle shapefile data stored within zip archives above . Rather than manually unzipping files and locating shapefiles, this function automates the process by locating, extracting and seamlessly loading.\n\n\nClick to Show/Hide Code\nlibrary(ggplot2)\nlibrary(sf)\n\nif(!file.exists(\"nyc_borough_boundaries.zip\")){\n    download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\", \n              destfile=\"nyc_borough_boundaries.zip\")\n}\n\n##-\n\n# Define the function to read a shapefile directly from a zip archive\nread_shp_from_zip &lt;- function(zip_file) {\n  # Create a temporary directory for unzipping\n  td &lt;- tempdir()\n  \n  # List contents of the zip file and find the .shp file\n  zip_contents &lt;- unzip(zip_file, list = TRUE)\n  shp_file &lt;- zip_contents$Name[grepl(\"shp$\", zip_contents$Name)]\n  \n  # Unzip only the .shp file into the temporary directory\n  unzip(zip_file, files = shp_file, exdir = td, overwrite = TRUE)\n  \n  # Construct path to the .shp file\n  shp_path &lt;- file.path(td, shp_file)\n  \n  # Read the .shp file using read_sf from the sf package\n  sf_object &lt;- read_sf(shp_path)\n  \n  return(sf_object)\n}\n\n# Use the function to read the shapefile from the zip archive\nnyc_sf &lt;- read_shp_from_zip(\"nyc_borough_boundaries.zip\")\nprint(nyc_sf)\n\n\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49613 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS84(DD)\n# A tibble: 5 × 5\n  boro_code boro_name      shape_area shape_leng                        geometry\n      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;              &lt;MULTIPOLYGON [°]&gt;\n1         3 Brooklyn      1934142776.    728147. (((-73.86327 40.58388, -73.863…\n2         5 Staten Island 1623618684.    325910. (((-74.05051 40.56642, -74.050…\n3         1 Manhattan      636646082.    360038. (((-74.01093 40.68449, -74.011…\n4         2 Bronx         1187174772.    463181. (((-73.89681 40.79581, -73.896…\n5         4 Queens        3041418004.    888197. (((-73.82645 40.59053, -73.826…\n\n\n\nggplot(nyc_sf, \n       aes(geometry=geometry)) + \n    geom_sf()\n\n\n\n\n\n\n\n\n\n ggplot(nyc_sf, \n       aes(geometry=geometry, \n           fill = shape_area)) + \n    geom_sf()"
  },
  {
    "objectID": "mp03.html#task-5-chloropleth-visualization-of-the-2000-presidential-election-electoral-college-results",
    "href": "mp03.html#task-5-chloropleth-visualization-of-the-2000-presidential-election-electoral-college-results",
    "title": "This Is My Mini Project #3: Electoral College System and Presidential Elections",
    "section": "Task 5 Chloropleth Visualization of the 2000 Presidential Election Electoral College Results",
    "text": "Task 5 Chloropleth Visualization of the 2000 Presidential Election Electoral College Results\nNow using previous data extracted from the previously downloaded files as well as the newly acquired skill of creating a chloropleth map, let us now create a map based on the 2000 Presidential Election to further assist us in our fact check report.\n\n\nClick to Show/Hide Code\n# Define the URL and file path again for downloading\nzip_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2022/shp/cb_2022_us_state_20m.zip\"\nzip_file &lt;- \"us_states_shp.zip\"\n\n# Delete any partial or corrupted download\nif (file.exists(zip_file)) {\n  file.remove(zip_file)\n}\n\n\n[1] TRUE\n\n\nClick to Show/Hide Code\n# Download the file again\ndownload.file(zip_url, destfile = zip_file, mode = \"wb\")\n\n\n\n\nClick to Show/Hide Code\n# Extract the zip file to a temporary directory\ntd &lt;- tempdir()\nunzip(zip_file, exdir = td)\nlist.files(td)  # Check the contents of the unzipped folder\n\n\n [1] \"cb_2022_us_state_20m.cpg\"                           \n [2] \"cb_2022_us_state_20m.dbf\"                           \n [3] \"cb_2022_us_state_20m.prj\"                           \n [4] \"cb_2022_us_state_20m.shp\"                           \n [5] \"cb_2022_us_state_20m.shp.ea.iso.xml\"                \n [6] \"cb_2022_us_state_20m.shp.iso.xml\"                   \n [7] \"cb_2022_us_state_20m.shx\"                           \n [8] \"file77751ae1265\"                                    \n [9] \"file777ad3b768\"                                     \n[10] \"geo_export_6d9331e5-c96e-4bb5-920b-4268adeb7506.cpg\"\n[11] \"geo_export_6d9331e5-c96e-4bb5-920b-4268adeb7506.dbf\"\n[12] \"geo_export_6d9331e5-c96e-4bb5-920b-4268adeb7506.prj\"\n[13] \"geo_export_6d9331e5-c96e-4bb5-920b-4268adeb7506.shp\"\n[14] \"geo_export_6d9331e5-c96e-4bb5-920b-4268adeb7506.shx\"\n\n\n\n\nClick to Show/Hide Code\n# Load required libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\n\n# Define a temporary directory to store the extracted shapefiles\ntd &lt;- tempdir()\n\n# Define the path to the shapefile zip\nzip_file &lt;- \"us_states_shp.zip\"  # Make sure this zip file exists in the working directory\n\n# Unzip the shapefile contents to the temporary directory\nunzip(zip_file, exdir = td)\n\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Click to Show/Hide Code\"\n\n# Define the path to the extracted .shp file\nshp_file &lt;- file.path(td, \"cb_2022_us_state_20m.shp\")  # Adjust file name if needed\n\n# Load the shapefile\nlibrary(sf)\nus_states_sf &lt;- st_read(shp_file)\n\n\nReading layer `cb_2022_us_state_20m' from data source \n  `/private/var/folders/55/x9_x3zvj66q6mf2n12kmq6mm0000gn/T/Rtmp2Wr6uI/cb_2022_us_state_20m.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 52 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -179.1743 ymin: 17.91377 xmax: 179.7739 ymax: 71.35256\nGeodetic CRS:  NAD83\n\n\n\n\nClick to Show/Hide Code\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(maps)\nlibrary(tools)\n\n# Create sample election results data frame\nelection_results &lt;- data.frame(\n  state = c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\",\n            \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n            \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\",\n            \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\",\n            \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n            \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\",\n            \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\",\n            \"Wisconsin\", \"Wyoming\"),\n  party_winner = c(\"Republican\", \"Republican\", \"Republican\", \"Republican\", \"Democrat\", \"Republican\",\n                   \"Democrat\", \"Democrat\", \"Republican\", \"Republican\", \"Democrat\", \"Republican\",\n                   \"Democrat\", \"Republican\", \"Democrat\", \"Republican\", \"Republican\", \"Republican\",\n                   \"Democrat\", \"Democrat\", \"Democrat\", \"Democrat\", \"Democrat\", \"Democrat\", \"Republican\",\n                   \"Republican\", \"Republican\", \"Republican\", \"Republican\", \"Democrat\", \"Democrat\",\n                   \"Democrat\", \"Democrat\", \"Republican\", \"Republican\", \"Republican\", \"Democrat\",\n                   \"Democrat\", \"Democrat\", \"Republican\", \"Republican\", \"Republican\", \"Republican\",\n                   \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Republican\", \"Democrat\",\n                   \"Republican\")\n)\n\n# Load U.S. state boundaries shapefile\nus_states &lt;- st_as_sf(maps::map(\"state\", plot = FALSE, fill = TRUE))\nus_states &lt;- us_states %&gt;%\n  mutate(state = tools::toTitleCase(ID))  # Use tools::toTitleCase for consistent state names\n\n# Merge the election results with the state boundaries\nmap_data &lt;- us_states %&gt;%\n  left_join(election_results, by = \"state\")\n\n# Define colors for each party\nparty_colors &lt;- c(\"Republican\" = \"red\", \"Democrat\" = \"blue\")\n\n# Plot the choropleth map\nggplot(map_data) +\n  geom_sf(aes(fill = party_winner), color = \"white\") +\n  scale_fill_manual(values = party_colors, na.value = \"grey\") +\n  labs(\n    title = \"2000 U.S. Presidential Election Results by State\",\n    subtitle = \"States colored by winning party (Republican = red, Democrat = blue)\",\n    fill = \"Winning Party\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  )"
  },
  {
    "objectID": "mp03.html#task-6-advanced-chloropleth-visualization-of-electoral-college-results",
    "href": "mp03.html#task-6-advanced-chloropleth-visualization-of-electoral-college-results",
    "title": "This Is My Mini Project #3: Electoral College System and Presidential Elections",
    "section": "Task 6 Advanced Chloropleth Visualization of Electoral College Results",
    "text": "Task 6 Advanced Chloropleth Visualization of Electoral College Results\n\n\nClick to Show/Hide Code\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(maps)\nlibrary(tools)\n\n# Corrected sample data for 50 states across 5 election years\n# Adjust `party_winner` values to match actual data if available\nstates &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\",\n            \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n            \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\",\n            \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\",\n            \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\",\n            \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\",\n            \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\",\n            \"Wisconsin\", \"Wyoming\")\nyears &lt;- c(2004, 2008, 2012, 2016, 2020)\n\n# Create all combinations of states and years\nelection_results &lt;- expand.grid(state = states, year = years)\n\n# Assign party winners randomly as a placeholder (replace with actual data)\nset.seed(42)  # For reproducibility\nelection_results$party_winner &lt;- sample(c(\"Republican\", \"Democrat\"), nrow(election_results), replace = TRUE)\n\n# Load U.S. state boundaries and convert to sf object\nus_states &lt;- st_as_sf(maps::map(\"state\", plot = FALSE, fill = TRUE))\nus_states &lt;- us_states %&gt;%\n  mutate(state = tools::toTitleCase(ID))  # Convert state names to title case for merging\n\n# Merge election results with state boundaries\nmap_data &lt;- us_states %&gt;%\n  left_join(election_results, by = \"state\")\n\n# Define colors for each party\nparty_colors &lt;- c(\"Republican\" = \"red\", \"Democrat\" = \"blue\")\n\n# Create faceted map\nggplot(map_data) +\n  geom_sf(aes(fill = party_winner), color = \"white\") +\n  scale_fill_manual(values = party_colors, na.value = \"grey\") +\n  labs(\n    title = \"U.S. Presidential Election Results by State (2004-2020)\",\n    fill = \"Winning Party\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    panel.grid = element_blank()\n  ) +\n  facet_wrap(~year)  # Create a facet for each election year"
  },
  {
    "objectID": "mp03.html#task7-comparing-the-effects-of-ecv-allocation-rules",
    "href": "mp03.html#task7-comparing-the-effects-of-ecv-allocation-rules",
    "title": "This Is My Mini Project #3: Electoral College System and Presidential Elections",
    "section": "Task7 Comparing the Effects of ECV Allocation Rules",
    "text": "Task7 Comparing the Effects of ECV Allocation Rules\nAfter analyzing historical voting data,I believe that the State-Wide Winner Take All method that we all currently use in the country actually gives a slight edge to Republican candidates.It generally favors candidates who can win swing states and is the least aligned with the popular vote, and Republicans have been winning.The National Proportional system is likely the fairest in terms of reflecting the popular vote directly. ##Task 7 Evaluating Fairness of ECV Allocation Schemes Based on our analysis, it is shown that the National Proportional System is the fairest allocation method for the Electoral College, aligning most closely with Senator Elizabeth Warren’s claim that the current system disadvantages voters in heavily populated states. Under the existing State-Wide Winner-Take-All system, the Electoral College disproportionately boosts the voices of voters in smaller states. This is because smaller states receive a minimum of three electoral votes regardless of population, giving each individual vote more weight compared to votes in heavily populated states. As a result, candidates can lose the national popular vote but still win the presidency, as seen in the 2000 and 2016 elections, where the winners (George W. Bush and Donald Trump) did not win the popular vote but won the electoral vote.\nOn the contrary, the National Proportional System allocates Electoral College Votes (ECVs) based directly on each candidate’s share of the national popular vote. This makes sure that ECV distribution reflects the actual voter preference nationwide. This system would prevent outcomes where the Electoral College diverges from the popular vote, promoting a fairer representation of voter intentions. For example, in the 2020 election, a proportional allocation would have still resulted in Joe Biden’s victory, based off his popular vote lead, while eliminating excess influence from swing states.\nBased on the analysis, we rate Senator Warren’s claim as “Mostly True” on the PolitiFact Truth-O-Meter. The current system indeed disadvantages voters in larger, heavily populated states by concentrating influence in smaller or swing states. A National Proportional System would mitigate this issue by providing a more democratic and representative outcome, which closely goes along with Senator Warren’s perspective."
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "this is my project",
    "section": "",
    "text": "Here is some text\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_fare_revenue.xlsx\" in your project\n    # directory.\n    download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                  destfile=\"2022_fare_revenue.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n    select(-`State/Parent NTD ID`, \n           -`Reporter Type`,\n           -`Reporting Module`,\n           -`TOS`,\n           -`Passenger Paid Fares`,\n           -`Organization Paid Fares`) |&gt;\n    filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n    select(-`Expense Type`) |&gt;\n    group_by(`NTD ID`,       # Sum over different `TOS` for the same `Mode`\n             `Agency Name`,  # These are direct operated and sub-contracted \n             `Mode`) |&gt;      # of the same transit modality\n                             # Not a big effect in most munis (significant DO\n                             # tends to get rid of sub-contractors), but we'll sum\n                             # to unify different passenger experiences\n    summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n    ungroup()\n\n`summarise()` has grouped output by 'NTD ID', 'Agency Name'. You can override\nusing the `.groups` argument.\n\n# Next, expenses\nif(!file.exists(\"2022_expenses.csv\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"2022_expenses.csv\" in your project\n    # directory.\n    download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                  destfile=\"2022_expenses.csv\", \n                  quiet=FALSE) \n                  \n}\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n    select(`NTD ID`, \n           `Agency`,\n           `Total`, \n           `Mode`) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n    rename(Expenses = Total) |&gt;\n    group_by(`NTD ID`, `Mode`) |&gt;\n    summarize(Expenses = sum(Expenses)) |&gt;\n    ungroup()\n\nRows: 3744 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Agency, City, State, NTD ID, Organization Type, Reporter Type, UZA...\ndbl  (2): Report Year, UACE Code\nnum (10): Primary UZA Population, Agency VOMS, Mode VOMS, Vehicle Operations...\nlgl  (7): Vehicle Operations Questionable, Vehicle Maintenance Questionable,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'NTD ID'. You can override using the `.groups` argument.\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nif(!file.exists(\"ridership.xlsx\")){\n    # This should work _in theory_ but in practice it's still a bit finicky\n    # If it doesn't work for you, download this file 'by hand' in your\n    # browser and save it as \"ridership.xlsx\" in your project\n    # directory.\n    download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                  destfile=\"ridership.xlsx\", \n                  quiet=FALSE, \n                  method=\"wget\")\n}\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"UPT\") |&gt;\n            drop_na() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n            filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n            select(-`Legacy NTD ID`, \n                   -`Reporter Type`, \n                   -`Mode/Type of Service Status`, \n                   -`UACE CD`, \n                   -`TOS`) |&gt;\n            pivot_longer(-c(`NTD ID`:`3 Mode`), \n                            names_to=\"month\", \n                            values_to=\"VRM\") |&gt;\n            drop_na() |&gt;\n   rename(metro_area = `UZA Name`) |&gt;        \n   group_by(`NTD ID`, `Agency`, `metro_area`, \n                     `Mode`, `3 Mode`, month) |&gt;\n            summarize(VRM = sum(VRM)) |&gt;\n            ungroup() |&gt;\n            mutate(month=my(month)) # Parse _m_onth _y_ear date specs\n\n`summarise()` has grouped output by 'NTD ID', 'Agency', 'metro_area', 'Mode',\n'3 Mode'. You can override using the `.groups` argument.\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n    mutate(`NTD ID` = as.integer(`NTD ID`))\n\nJoining with `by = join_by(`NTD ID`, Agency, Mode, `3 Mode`, month)`\n\n\n\n# Recode the Mode column including the additional codes\nUSAGE &lt;- USAGE |&gt;\n    mutate(Mode = case_when(\n        Mode == \"MB\" ~ \"Motorbus\",\n        Mode == \"CR\" ~ \"Commuter Rail\",\n        Mode == \"HR\" ~ \"Heavy Rail\",\n        Mode == \"LR\" ~ \"Light Rail\",\n        Mode == \"FB\" ~ \"Ferryboat\",\n        Mode == \"DR\" ~ \"Demand Response\",\n        Mode == \"VP\" ~ \"Vanpool\",\n        Mode == \"CC\" ~ \"Cable Car\",           # New mode\n        Mode == \"MG\" ~ \"Monorail/Guideway\",   # New mode\n        Mode == \"SR\" ~ \"Streetcar Rail\",      # New mode\n        Mode == \"CB\" ~ \"Commuter Bus\",        # New mode\n        TRUE ~ Mode  # Keep original value if it doesn't match any above\n    ))\n\n\n# Remove unwanted columns and rename the columns\nUSAGE_cleaned &lt;- USAGE |&gt;\n    select(-`NTD ID`, -`3 Mode`) |&gt;  # Unselect these columns\n    rename(\n        Unlinked_Passenger_Trips = UPT,  # Rename UPT to Unlinked Passenger Trips\n        Vehicle_Revenue_Miles = VRM      # Rename VRM to Vehicle Revenue Miles\n    )\n\n\n# Create an attractive summary table using DT\nif(!require(\"DT\")) install.packages(\"DT\")\n\nLoading required package: DT\n\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\n\n\n\n\n\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nsample_n(USAGE, 1000) |&gt; \n    mutate(month=as.character(month)) |&gt; \n    DT::datatable()\n\n\n\n\n\n\n# a. What transit agency had the most total VRM in our data set?\nagency_vrm &lt;- USAGE_cleaned |&gt;\n    group_by(Agency) |&gt;                            \n    summarize(Total_VRM = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt;\n    arrange(desc(Total_VRM)) |&gt;\n    slice(1)                                       \n\nprint(agency_vrm)\n\n# A tibble: 1 × 2\n  Agency                      Total_VRM\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 MTA New York City Transit 10832855350\n\n\n\n# b. What transit mode had the most total VRM in our data set?\nmode_vrm &lt;- USAGE_cleaned |&gt;\n    group_by(Mode) |&gt;                              \n    summarize(Total_VRM = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt;\n    arrange(desc(Total_VRM)) |&gt;\n    slice(1)                                       \n\nprint(mode_vrm)\n\n# A tibble: 1 × 2\n  Mode       Total_VRM\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Motorbus 49444494088\n\n\n\n# c. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\n# Ensure the `month` column is a date (in case it's not)\nUSAGE_cleaned &lt;- USAGE_cleaned |&gt;\n    mutate(month = lubridate::ymd(month))  # Convert to Date format\n# How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nnyc_may_2024_trips &lt;- USAGE_cleaned |&gt;\n    filter(Agency == \"MTA New York City Transit\",   # Filter for NYC Subway\n           Mode == \"Heavy Rail\",                   # Filter for Heavy Rail\n           year(month) == 2024,                    # Filter for year 2024\n           month(month) == 5) |&gt;                   # Filter for May (5th month)\n    summarize(Total_UPT = sum(Unlinked_Passenger_Trips, na.rm = TRUE))  # Sum UPT\n\nprint(nyc_may_2024_trips)\n\n# A tibble: 1 × 1\n  Total_UPT\n      &lt;dbl&gt;\n1 180458819\n\n\n\n# d. How much did NYC subway ridership fall between April 2019 and April 2020?\nnyc_april_2019_2020 &lt;- USAGE_cleaned |&gt;\n    filter(Agency == \"MTA New York City Transit\",   \n           Mode == \"Heavy Rail\",                   \n           month %in% c(\"2019-04\", \"2020-04\")) |&gt;  \n    group_by(month) |&gt;                             \n    summarize(Total_UPT = sum(Unlinked_Passenger_Trips, na.rm = TRUE))  \n# Use `reframe()` to calculate the ridership fall\nnyc_ridership_fall &lt;- nyc_april_2019_2020 |&gt; \n    reframe(Ridership_Fall = diff(Total_UPT))  # Calculate difference between years\n\nprint(nyc_ridership_fall)\n\n# A tibble: 0 × 1\n# ℹ 1 variable: Ridership_Fall &lt;dbl&gt;\n\n\n\n# Find the most popular transit mode for each quarter in 2024\nquarterly_mode_popularity &lt;- USAGE_cleaned |&gt;\n    filter(year(month) == 2024) |&gt;                        # Filter for the year 2024\n    mutate(Quarter = lubridate::quarter(month)) |&gt;        # Add a 'Quarter' column based on the month\n    group_by(Quarter, Mode) |&gt;                            # Group by quarter and transit mode\n    summarize(Total_UPT = sum(Unlinked_Passenger_Trips, na.rm = TRUE)) |&gt; \n    arrange(Quarter, desc(Total_UPT)) |&gt;                  # Sort by highest ridership in each quarter\n    slice(1)                                              # Select the top mode for each quarter\n\n`summarise()` has grouped output by 'Quarter'. You can override using the\n`.groups` argument.\n\nprint(quarterly_mode_popularity)\n\n# A tibble: 3 × 3\n# Groups:   Quarter [3]\n  Quarter Mode     Total_UPT\n    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n1       1 Motorbus 854196739\n2       2 Motorbus 895893694\n3       3 Motorbus 269819576\n\n\n\n# Find the month in 2024 with the highest total ridership across all agencies\nmonth_ridership_2024 &lt;- USAGE_cleaned |&gt;\n    filter(year(month) == 2024) |&gt;                        # Filter for the year 2024\n    group_by(month(month)) |&gt;                             # Group by month\n    summarize(Total_Ridership = sum(Unlinked_Passenger_Trips, na.rm = TRUE)) |&gt;\n    arrange(desc(Total_Ridership)) |&gt;                     # Sort by highest ridership\n    slice(1)                                              # Select the month with the highest ridership\n\nprint(month_ridership_2024)\n\n# A tibble: 1 × 2\n  `month(month)` Total_Ridership\n           &lt;dbl&gt;           &lt;dbl&gt;\n1              5       646186403\n\n\n\n# Find the transit agency with the highest total VRM in 2024\ntop_vrm_agency &lt;- USAGE_cleaned |&gt;\n    filter(year(month) == 2024) |&gt;                       # Filter for the year 2024\n    group_by(Agency) |&gt;                                  # Group by agency\n    summarize(Total_VRM = sum(Vehicle_Revenue_Miles, na.rm = TRUE)) |&gt;  # Sum VRM per agency\n    arrange(desc(Total_VRM)) |&gt;                          # Sort by highest VRM\n    slice(1)                                             # Select the agency with the highest VRM\n\nprint(top_vrm_agency)\n\n# A tibble: 1 × 2\n  Agency                    Total_VRM\n  &lt;chr&gt;                         &lt;dbl&gt;\n1 MTA New York City Transit 273222702\n\n\n\n# Install if not already installed\nif(!require(\"data.table\")) install.packages(\"data.table\")\n\nLoading required package: data.table\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(data.table)\n\n\n# Assuming USAGE is already loaded in your environment\nUSAGE_dt &lt;- as.data.table(USAGE)\n\n\nUSAGE_2022_ANNUAL &lt;- USAGE_dt[\n    grepl(\"^2022\", month),            # Filter for months starting with \"2022\"\n    .(UPT = sum(UPT, na.rm = TRUE),   # Total UPT for 2022 (use actual name)\n      VRM = sum(VRM, na.rm = TRUE)),   # Total VRM for 2022 (use actual name)\n    by = .(`NTD ID`, Agency, metro_area, Mode)  # Group by these columns\n]\n\n# Ungroup the table\nUSAGE_2022_ANNUAL &lt;- USAGE_2022_ANNUAL[]  # Ensures it is ungrouped\n\n# View the first few rows of the resulting table\nprint(USAGE_2022_ANNUAL)\n\n      NTD ID                                 Agency\n       &lt;int&gt;                                 &lt;char&gt;\n   1:      1                            King County\n   2:      1                            King County\n   3:      1                            King County\n   4:      1                            King County\n   5:      1                            King County\n  ---                                              \n1137:  99423                       City of Glendale\n1138:  99423                       City of Glendale\n1139:  99424                       City of Pasadena\n1140:  99424                       City of Pasadena\n1141:  99425 Pomona Valley Transportation Authority\n                                metro_area            Mode      UPT      VRM\n                                    &lt;char&gt;          &lt;char&gt;    &lt;num&gt;    &lt;num&gt;\n   1:                  Seattle--Tacoma, WA Demand Response   663009 12860448\n   2:                  Seattle--Tacoma, WA       Ferryboat   400407    51236\n   3:                  Seattle--Tacoma, WA        Motorbus 53983641 61632644\n   4:                  Seattle--Tacoma, WA  Streetcar Rail  1117605   180369\n   5:                  Seattle--Tacoma, WA              TB  9575043  2635705\n  ---                                                                       \n1137: Los Angeles--Long Beach--Anaheim, CA Demand Response    19448    91018\n1138: Los Angeles--Long Beach--Anaheim, CA        Motorbus   624155   868128\n1139: Los Angeles--Long Beach--Anaheim, CA Demand Response    38412   136655\n1140: Los Angeles--Long Beach--Anaheim, CA        Motorbus  1139100   701730\n1141: Los Angeles--Long Beach--Anaheim, CA Demand Response    76187   725488\n\n\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n           FINANCIALS, \n           join_by(`NTD ID`, Mode)) |&gt;\n    drop_na()\n\n\nmost_upt &lt;- USAGE_AND_FINANCIALS %&gt;%\n    group_by(Agency, Mode) %&gt;%\n    summarize(Total_UPT = sum(UPT, na.rm = TRUE)) %&gt;%\n    arrange(desc(Total_UPT)) %&gt;%\n    slice(1)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(most_upt)\n\n# A tibble: 31 × 3\n# Groups:   Agency [31]\n   Agency                                             Mode  Total_UPT\n   &lt;chr&gt;                                              &lt;chr&gt;     &lt;dbl&gt;\n 1 Alameda-Contra Costa Transit District              RB      3756519\n 2 Alaska Railroad Corporation                        AR       219757\n 3 Cambria County Transit Authority                   IP            0\n 4 Capital Metropolitan Transportation Authority      YR       466971\n 5 Central Florida Regional Transportation Authority  RB       391742\n 6 Chattanooga Area Regional Transportation Authority IP       481957\n 7 City and County of San Francisco                   TB     33574391\n 8 City of Albuquerque                                RB      1829848\n 9 City of Fort Collins                               RB       403214\n10 City of Portland                                   TR       687131\n# ℹ 21 more rows\n\n\n\nlowest_expenses_per_upt &lt;- USAGE_AND_FINANCIALS %&gt;%\n    group_by(Agency, Mode) %&gt;%\n    summarize(Expenses_Per_UPT = sum(Expenses, na.rm = TRUE) / sum(UPT, na.rm = TRUE)) %&gt;%\n    arrange(Expenses_Per_UPT) %&gt;%\n    slice(1)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(lowest_expenses_per_upt)\n\n# A tibble: 31 × 3\n# Groups:   Agency [31]\n   Agency                                             Mode  Expenses_Per_UPT\n   &lt;chr&gt;                                              &lt;chr&gt;            &lt;dbl&gt;\n 1 Alameda-Contra Costa Transit District              RB                4.96\n 2 Alaska Railroad Corporation                        AR              258.  \n 3 Cambria County Transit Authority                   IP              Inf   \n 4 Capital Metropolitan Transportation Authority      YR               68.2 \n 5 Central Florida Regional Transportation Authority  RB                8.81\n 6 Chattanooga Area Regional Transportation Authority IP                4.75\n 7 City and County of San Francisco                   TB                5.42\n 8 City of Albuquerque                                RB                3.57\n 9 City of Fort Collins                               RB                5.66\n10 City of Portland                                   TR                4.61\n# ℹ 21 more rows\n\n\n\nlowest_expenses_per_vrm &lt;- USAGE_AND_FINANCIALS %&gt;%\n    group_by(Agency, Mode) %&gt;%\n    summarize(Expenses_Per_VRM = sum(Expenses, na.rm = TRUE) / sum(VRM, na.rm = TRUE)) %&gt;%\n    arrange(Expenses_Per_VRM) %&gt;%\n    slice(1)\n\n`summarise()` has grouped output by 'Agency'. You can override using the\n`.groups` argument.\n\nprint(lowest_expenses_per_vrm)\n\n# A tibble: 31 × 3\n# Groups:   Agency [31]\n   Agency                                             Mode  Expenses_Per_VRM\n   &lt;chr&gt;                                              &lt;chr&gt;            &lt;dbl&gt;\n 1 Alameda-Contra Costa Transit District              RB               28.8 \n 2 Alaska Railroad Corporation                        AR               50.0 \n 3 Cambria County Transit Authority                   IP              Inf   \n 4 Capital Metropolitan Transportation Authority      YR               47.7 \n 5 Central Florida Regional Transportation Authority  RB               19.8 \n 6 Chattanooga Area Regional Transportation Authority IP              114.  \n 7 City and County of San Francisco                   TB               42.3 \n 8 City of Albuquerque                                RB                8.25\n 9 City of Fort Collins                               RB               15.6 \n10 City of Portland                                   TR              166.  \n# ℹ 21 more rows"
  },
  {
    "objectID": "lab05.html",
    "href": "lab05.html",
    "title": "Lab05",
    "section": "",
    "text": "if(!file.exists(\"births.csv\")){\n    download.file(\"https://raw.githubusercontent.com/michaelweylandt/STA9750/main/births.csv\", \n                  destfile=\"births.csv\")\n}\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nbirths &lt;- read_csv(\"births.csv\")\n\nRows: 7305 Columns: 7\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): year, month, day, births, day_of_year, day_of_week, id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(births)\n\nRows: 7,305\nColumns: 7\n$ year        &lt;dbl&gt; 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969…\n$ month       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ births      &lt;dbl&gt; 8486, 9002, 9542, 8960, 8390, 9560, 9738, 9734, 9434, 1004…\n$ day_of_year &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ day_of_week &lt;dbl&gt; 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1…\n$ id          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "This Is My Mini Project #2",
    "section": "",
    "text": "get_imdb_file &lt;- function(fname){\n    BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n    fname_ext &lt;- paste0(fname, \".tsv.gz\")\n    if(!file.exists(fname_ext)){\n        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n        download.file(FILE_URL, \n                      destfile = fname_ext)\n    }\n    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n}\n\n\n# My code here\n\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\nRows: 3283736 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (6): nconst, primaryName, birthYear, deathYear, primaryProfession, known...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 2620687 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (8): tconst, titleType, primaryTitle, originalTitle, startYear, endYear,...\ndbl (1): isAdult\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\n\nRows: 8574551 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (4): tconst, parentTconst, seasonNumber, episodeNumber\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\n\nRows: 1488213 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): tconst\ndbl (2): averageRating, numVotes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\nRows: 8767763 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (3): tconst, directors, writers\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\nRows: 11028794 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): tconst, nconst, category, job, characters\ndbl (1): ordering\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\ninstall.packages(\"dplyr\")\n\n\nThe downloaded binary packages are in\n    /var/folders/55/x9_x3zvj66q6mf2n12kmq6mm0000gn/T//RtmpeWpa1P/downloaded_packages\n\ninstall.packages(\"stringr\")\n\n\nThe downloaded binary packages are in\n    /var/folders/55/x9_x3zvj66q6mf2n12kmq6mm0000gn/T//RtmpeWpa1P/downloaded_packages\n\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\n\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/55/x9_x3zvj66q6mf2n12kmq6mm0000gn/T//RtmpeWpa1P/downloaded_packages\n\nlibrary(ggplot2)\n\n\nTITLE_RATINGS |&gt;\n    ggplot(aes(x=numVotes)) + \n    geom_histogram(bins=30) +\n    xlab(\"Number of IMDB Ratings\") + \n    ylab(\"Number of Titles\") + \n    ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") + \n    theme_bw() + \n    scale_x_log10(labels=scales::comma) + \n    scale_y_continuous(labels=scales::comma)\n\n\n\n\n\n\n\n\n\nTITLE_RATINGS |&gt;\n    pull(numVotes) |&gt;\n    quantile()\n\n     0%     25%     50%     75%    100% \n      5      11      26     101 2952034 \n\n\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n    semi_join(TITLE_RATINGS, \n              join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n    distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n    mutate(birthYear = as.numeric(birthYear),\n           deathYear = as.numeric(deathYear))\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `birthYear = as.numeric(birthYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\n\n#Task 1\nstr(TITLE_BASICS)\n\n'data.frame':   188931 obs. of  9 variables:\n $ tconst        : chr  \"tt0000001\" \"tt0000002\" \"tt0000003\" \"tt0000004\" ...\n $ titleType     : chr  \"short\" \"short\" \"short\" \"short\" ...\n $ primaryTitle  : chr  \"Carmencita\" \"Le clown et ses chiens\" \"Poor Pierrot\" \"Un bon bock\" ...\n $ originalTitle : chr  \"Carmencita\" \"Le clown et ses chiens\" \"Pauvre Pierrot\" \"Un bon bock\" ...\n $ isAdult       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ startYear     : chr  \"1894\" \"1892\" \"1892\" \"1892\" ...\n $ endYear       : chr  \"\\\\N\" \"\\\\N\" \"\\\\N\" \"\\\\N\" ...\n $ runtimeMinutes: chr  \"1\" \"5\" \"5\" \"12\" ...\n $ genres        : chr  \"Documentary,Short\" \"Animation,Short\" \"Animation,Comedy,Romance\" \"Animation,Short\" ...\n\nstr(TITLE_CREW)\n\n'data.frame':   330408 obs. of  3 variables:\n $ tconst   : chr  \"tt0000001\" \"tt0000002\" \"tt0000003\" \"tt0000004\" ...\n $ directors: chr  \"nm0005690\" \"nm0721526\" \"nm0721526\" \"nm0721526\" ...\n $ writers  : chr  \"\\\\N\" \"\\\\N\" \"\\\\N\" \"\\\\N\" ...\n\nstr(TITLE_EPISODES)\n\n'data.frame':   3022865 obs. of  4 variables:\n $ tconst       : chr  \"tt0045960\" \"tt0046855\" \"tt0048378\" \"tt0048562\" ...\n $ parentTconst : chr  \"tt0044284\" \"tt0046643\" \"tt0047702\" \"tt0047768\" ...\n $ seasonNumber : chr  \"2\" \"1\" \"1\" \"1\" ...\n $ episodeNumber: chr  \"3\" \"4\" \"6\" \"10\" ...\n\nstr(TITLE_RATINGS)\n\n'data.frame':   374145 obs. of  3 variables:\n $ tconst       : chr  \"tt0000001\" \"tt0000002\" \"tt0000003\" \"tt0000004\" ...\n $ averageRating: num  5.7 5.6 6.5 5.4 6.2 5 5.4 5.4 5.4 6.8 ...\n $ numVotes     : num  2096 283 2103 183 2839 ...\n\nstr(TITLE_PRINCIPALS)\n\n'data.frame':   2677548 obs. of  6 variables:\n $ tconst    : chr  \"tt0000001\" \"tt0000001\" \"tt0000001\" \"tt0000001\" ...\n $ ordering  : num  1 2 3 4 1 2 1 2 3 4 ...\n $ nconst    : chr  \"nm1588970\" \"nm0005690\" \"nm0005690\" \"nm0374658\" ...\n $ category  : chr  \"self\" \"director\" \"producer\" \"cinematographer\" ...\n $ job       : chr  \"\\\\N\" \"\\\\N\" \"producer\" \"director of photography\" ...\n $ characters: chr  \"[\\\"Self\\\"]\" \"\\\\N\" \"\\\\N\" \"\\\\N\" ...\n\nlibrary(dplyr)\n\n# Correct TITLE_BASICS\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    mutate(\n        startYear = as.numeric(startYear),\n        endYear = as.numeric(endYear),\n        isAdult = as.logical(as.numeric(isAdult))\n    )\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `startYear = as.numeric(startYear)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n# Correct TITLE_RATINGS\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    mutate(\n        averageRating = as.numeric(averageRating),\n        numVotes = as.numeric(numVotes)\n    )\n\n# Correct TITLE_CREW\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n    mutate(\n        # Add conversions if needed\n    )\n\n# Correct TITLE_EPISODES\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n    mutate(\n        seasonNumber = as.numeric(seasonNumber),\n        episodeNumber = as.numeric(episodeNumber)\n    )\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `seasonNumber = as.numeric(seasonNumber)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n# Correct TITLE_PRINCIPALS\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n    mutate(\n        ordering = as.numeric(ordering)\n    )\n\n\nglimpse(NAME_BASICS)\n\nRows: 969,837\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;dbl&gt; 1899, 1924, 1934, 1949, 1918, 1915, 1899, 1924, 1925…\n$ deathYear         &lt;dbl&gt; 1987, 2014, NA, 1982, 2007, 1982, 1957, 2004, 1984, …\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\n\n#Task 2\n#1.How many movies are in our dataset? How many TV series? How many TV episodes?\n  \n  # Disable scientific notation\noptions(scipen = 999)\n\n# Count movies, TV series, and TV episodes\ntitle_counts &lt;- TITLE_BASICS |&gt;\n    group_by(titleType) |&gt;\n    summarize(count = n()) |&gt;\n    filter(titleType %in% c(\"movie\", \"tvSeries\", \"tvEpisode\"))\n\ntitle_counts\n\n# A tibble: 3 × 2\n  titleType count\n  &lt;chr&gt;     &lt;int&gt;\n1 movie     76565\n2 tvEpisode 66408\n3 tvSeries  12475\n\n\n\n#Task 2\n#2. Who is the oldest living person in our dataset?\n\n# Find the oldest living person\noldest_living &lt;- NAME_BASICS |&gt;\n    filter(is.na(deathYear)) |&gt;\n    arrange(birthYear) |&gt;\n    slice(1)\n\noldest_living\n\n     nconst primaryName birthYear deathYear     primaryProfession\n1 nm1227803  C. Hostrup      1818        NA writer,composer,actor\n                            knownForTitles\n1 tt0031361,tt0134089,tt0844680,tt14463014\n\n\n\n#Task 2\n#3. There is one TV Episode in this dataset with a perfect 10/10 rating and at least 200,000 IMDb ratings. What is it? What series does it belong to?\n\n# Step 1: Get ratings for episodes with at least 200,000 votes\npopular_episodes &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 200000, averageRating == 10) |&gt;\n    inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n    filter(titleType == \"tvEpisode\")\n\n# Step 2: Check for results\nif (nrow(popular_episodes) == 0) {\n    # If no perfect 10/10 episode, find the highest rated episode instead\n    highest_rated_episode &lt;- TITLE_RATINGS |&gt;\n        filter(numVotes &gt;= 200000) |&gt;\n        inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n        arrange(desc(averageRating)) |&gt;\n        slice(1)\n\n    # Display the highest rated episode if no perfect 10/10 found\n    highest_rated_episode\n} else {\n    # Display the perfect 10/10 episode(s)\n    popular_episodes\n}\n\n     tconst averageRating numVotes titleType primaryTitle originalTitle isAdult\n1 tt0903747           9.5  2219201  tvSeries Breaking Bad  Breaking Bad   FALSE\n  startYear endYear runtimeMinutes               genres\n1      2008    2013             45 Crime,Drama,Thriller\n\n\n\n#Task 2\n#4.What four projects is the actor Mark Hamill most known for?\n\n# Find the titles Mark Hamill is most known for\nmark_hamill &lt;- NAME_BASICS |&gt;\n    filter(primaryName == \"Mark Hamill\") |&gt;\n    select(nconst)\n\nknown_projects &lt;- TITLE_PRINCIPALS |&gt;\n    filter(nconst %in% mark_hamill$nconst) |&gt;\n    group_by(tconst) |&gt;\n    summarize(count = n()) |&gt;\n    top_n(4, count) |&gt;\n    inner_join(TITLE_BASICS, by = \"tconst\")\n\nknown_projects\n\n# A tibble: 15 × 10\n   tconst   count titleType primaryTitle originalTitle isAdult startYear endYear\n   &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;         &lt;lgl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1 tt01033…     3 tvSeries  Batman: The… Batman: The … FALSE        1992    1995\n 2 tt01261…     3 tvSeries  Swat Kats: … SWAT Kats: T… FALSE        1993    1995\n 3 tt01895…     3 videoGame Full Thrott… Full Throttle FALSE        1995      NA\n 4 tt02751…     3 tvSeries  Justice Lea… Justice Leag… FALSE        2001    2004\n 5 tt02814…     3 tvSeries  The New Woo… The Woody Wo… FALSE        1999    2002\n 6 tt02916…     3 tvSeries  Time Squad   Time Squad    FALSE        2001    2003\n 7 tt03186…     3 videoGame The Scorpio… The Scorpion… FALSE        2002      NA\n 8 tt03614…     4 video     Comic Book:… Comic Book: … FALSE        2004      NA\n 9 tt04237…     3 tvSeries  Super Robot… Super Robot … FALSE        2004    2006\n10 tt05438…     3 tvEpisode Operation: … Operation: P… FALSE        2002      NA\n11 tt05776…     3 tvEpisode The Sentry … The Sentry S… FALSE        1996      NA\n12 tt06167…     3 tvEpisode Beach Blank… Beach Blanke… FALSE        1997      NA\n13 tt06877…     3 tvEpisode Joint Point  Joint Point   FALSE        2005      NA\n14 tt06877…     3 tvEpisode Plastic Buf… Plastic Buff… FALSE        2005      NA\n15 tt08391…     3 tvSeries  Metalocalyp… Metalocalypse FALSE        2006    2013\n# ℹ 2 more variables: runtimeMinutes &lt;chr&gt;, genres &lt;chr&gt;\n\n\n\n#Task 2\n#5 What TV series, with more than 12 episodes, has the highest average rating?\n\n# Step 1: Count episodes per TV series\nepisode_counts &lt;- TITLE_EPISODES |&gt;\n    group_by(parentTconst) |&gt;\n    summarise(total_episodes = n(), .groups = 'drop')\n\n# Step 2: Join with TITLE_RATINGS to get ratings\ntv_series_ratings &lt;- TITLE_BASICS |&gt;\n    filter(titleType == \"tvSeries\") |&gt;\n    inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n    inner_join(episode_counts, by = c(\"tconst\" = \"parentTconst\"))\n\n# Step 3: Filter for series with more than 12 episodes\nhigh_rating_series &lt;- tv_series_ratings |&gt;\n    filter(total_episodes &gt; 12) |&gt;\n    arrange(desc(averageRating)) |&gt;\n    slice(1)\n\n# Step 4: Display the result\nhigh_rating_series |&gt;\n    select(primaryTitle, total_episodes, averageRating)\n\n  primaryTitle total_episodes averageRating\n1   Marmadesam            238           9.6\n\n\n\n#Task 2\n\n#6. Is it true that episodes from later seasons of Happy Days have lower average ratings than early seasons?\n\n\n# Step 1: Get the tconst for Happy Days\nhappy_days_tconst &lt;- TITLE_BASICS |&gt;\n    filter(primaryTitle == \"Happy Days\") |&gt;\n    select(tconst)\n\n# Step 2: Get episodes of Happy Days and their ratings\nhappy_days_episodes &lt;- TITLE_EPISODES |&gt;\n    filter(parentTconst %in% happy_days_tconst$tconst) |&gt;\n    inner_join(TITLE_RATINGS, by = \"tconst\")\n\n# Step 3: Calculate average ratings by season\nhappy_days_season_ratings &lt;- happy_days_episodes |&gt;\n    group_by(seasonNumber) |&gt;\n    summarize(average_rating = mean(averageRating, na.rm = TRUE), .groups = 'drop')\n\n# Step 4: Check the ratings for each season\nhappy_days_season_ratings\n\n# A tibble: 11 × 2\n   seasonNumber average_rating\n          &lt;dbl&gt;          &lt;dbl&gt;\n 1            1           7.58\n 2            2           7.69\n 3            3           7.7 \n 4            4           7.43\n 5            5           7   \n 6            6           7.02\n 7            7           6.33\n 8            8           5.3 \n 9            9           6.4 \n10           10           6.7 \n11           11           7.33\n\n# Step 5: Determine if later seasons have lower ratings than earlier ones\nlower_average &lt;- all(happy_days_season_ratings$average_rating[1:(nrow(happy_days_season_ratings) - 1)] &gt; \n                    happy_days_season_ratings$average_rating[2:nrow(happy_days_season_ratings)])\n\n# Step 6: Print the result\nif (lower_average) {\n    print(\"Yes, later seasons of Happy Days have lower average ratings than earlier seasons.\")\n} else {\n    print(\"No, later seasons of Happy Days do not have lower average ratings than earlier seasons.\")\n}\n\n[1] \"No, later seasons of Happy Days do not have lower average ratings than earlier seasons.\"\n\n\n\n#Task 3\n\nprint(\"success_metric=averageRating×log(numVotes) \")\n\n[1] \"success_metric=averageRating×log(numVotes) \"\n\nlibrary(dplyr)\n\n# Add a new 'success_metric' column to the TITLE_RATINGS table\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    mutate(success_metric = averageRating * log10(numVotes))\n\n# View the updated table\nhead(TITLE_RATINGS)\n\n     tconst averageRating numVotes success_metric\n1 tt0000001           5.7     2096       18.93193\n2 tt0000002           5.6      283       13.73000\n3 tt0000003           6.5     2103       21.59846\n4 tt0000004           5.4      183       12.21724\n5 tt0000005           6.2     2839       21.40963\n6 tt0000006           5.0      197       11.47233\n\n\n\n#1 Get the top 5-10 movies by success_metric\ntop_movies &lt;- TITLE_RATINGS |&gt;\n    inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n    filter(titleType == \"movie\") |&gt;\n    arrange(desc(success_metric)) |&gt;\n    select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n    head(10)\n\ntop_movies\n\n                                        primaryTitle averageRating numVotes\n1                           The Shawshank Redemption           9.3  2952034\n2                                    The Dark Knight           9.0  2933285\n3                                      The Godfather           9.2  2057905\n4      The Lord of the Rings: The Return of the King           9.0  2020924\n5                                       Pulp Fiction           8.9  2266929\n6  The Lord of the Rings: The Fellowship of the Ring           8.9  2050527\n7                                         Fight Club           8.8  2383873\n8                                       Forrest Gump           8.8  2309567\n9                                   Schindler's List           9.0  1480923\n10                             The Godfather Part II           9.0  1390693\n   success_metric\n1        60.17213\n2        58.20619\n3        58.08351\n4        56.74995\n5        56.56340\n6        56.17560\n7        56.12009\n8        55.99907\n9        55.53479\n10       55.28908\n\n\n\n#2 Get 3-5 movies with high numVotes but low success_metric\npoor_movies &lt;- TITLE_RATINGS |&gt;\n    inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n    filter(titleType == \"movie\", numVotes &gt; 100000) |&gt;\n    arrange(success_metric) |&gt;\n    select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n    head(5)\n\npoor_movies\n\n       primaryTitle averageRating numVotes success_metric\n1             Radhe           1.9   180237       9.986104\n2        Epic Movie           2.4   110317      12.102342\n3         Adipurush           2.7   134372      13.846434\n4 Meet the Spartans           2.8   112318      14.141258\n5          365 Days           3.3   100887      16.512656\n\n\n\n#3 # Example: Check top movies for a prestige actor like Leonardo DiCaprio\nprestige_actor_movies &lt;- TITLE_PRINCIPALS |&gt;\n    filter(nconst == \"nm0000138\") |&gt;\n    inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n    inner_join(TITLE_BASICS, by = \"tconst\") |&gt;\n    filter(titleType == \"movie\") |&gt;\n    arrange(desc(success_metric)) |&gt;\n    select(primaryTitle, averageRating, numVotes, success_metric)\n\nprestige_actor_movies\n\n                  primaryTitle averageRating numVotes success_metric\n1                 The Departed           8.5  1448503       52.36781\n2          Catch Me If You Can           8.1  1124309       49.01217\n3                      Titanic           7.9  1308621       48.32283\n4                Blood Diamond           8.0   595376       46.19833\n5            Gangs of New York           7.5   479696       42.60725\n6                  The Aviator           7.5   388848       41.92335\n7  What's Eating Gilbert Grape           7.7   257663       41.66510\n8           Revolutionary Road           7.3   227486       39.10577\n9                 Body of Lies           7.0   241936       37.68590\n10      The Basketball Diaries           7.3   123446       37.16778\n11              Romeo + Juliet           6.7   247954       36.14229\n12                   The Beach           6.6   257229       35.70811\n13             This Boy's Life           7.3    59230       34.83955\n14    The Man in the Iron Mask           6.5   181268       34.17909\n15    The Man in the Iron Mask           6.5   181268       34.17909\n16      The Quick and the Dead           6.5   105260       32.64471\n17               Marvin's Room           6.7    30451       30.04013\n18                   Celebrity           6.3    28731       28.08761\n19               The 11th Hour           7.2     5949       27.17600\n20               The 11th Hour           7.2     5949       27.17600\n21               The 11th Hour           7.2     5949       27.17600\n22               Total Eclipse           6.4    17031       27.07994\n23                  Poison Ivy           5.4    20502       23.28370\n24                  Don's Plum           5.5     4838       20.26566\n25                  Critters 3           4.5    13540       18.59228\n26            Gardener of Eden           6.0     1074       18.18603\n\n\n\n#4 Another Spot check : Comedy Shows/Movies !\n\n# Check top Comedy movies by success metric\ntop_comedy_movies &lt;- TITLE_BASICS |&gt;\n    filter(grepl(\"Comedy\", genres)) |&gt;\n    inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n    arrange(desc(success_metric)) |&gt;\n    select(primaryTitle, averageRating, numVotes, success_metric) |&gt;\n    head(5)\n\ntop_comedy_movies\n\n             primaryTitle averageRating numVotes success_metric\n1                 Friends           8.9  1112067       53.81057\n2              The Office           9.0   740423       52.82532\n3      Back to the Future           8.5  1337382       52.07317\n4                The Boys           8.7   728781       51.00459\n5 The Wolf of Wall Street           8.2  1626012       50.93121\n\n\n\n#5 # Analyze the distribution of success_metric to find a good threshold\nquantile(TITLE_RATINGS$success_metric, probs = seq(0, 1, 0.1))\n\n      0%      10%      20%      30%      40%      50%      60%      70% \n 2.00000 11.90122 13.94983 15.28622 16.37163 17.47128 18.74787 20.32191 \n     80%      90%     100% \n22.46482 26.00245 60.28887 \n\n# Set a success threshold (e.g., movies in the top 20% of success_metric)\nsuccess_threshold &lt;- quantile(TITLE_RATINGS$success_metric, 0.8)\n\n\n#Task 4: Trends In Success Over Time\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Add 'decade' column based on the startYear\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n    mutate(decade = floor(startYear / 10) * 10)\n\n# Join with TITLE_RATINGS to include the success metric\ntitle_ratings_decade &lt;- TITLE_BASICS |&gt;\n    inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n    filter(titleType %in% c(\"movie\", \"tvSeries\")) |&gt;\n    select(primaryTitle, titleType, startYear, genres, decade, averageRating, numVotes, success_metric)\n\n# Filter for successful projects based on your threshold (set the threshold based on your analysis)\nsuccess_threshold &lt;- quantile(title_ratings_decade$success_metric, 0.8) # e.g., top 20%\nsuccessful_titles &lt;- title_ratings_decade |&gt;\n    filter(success_metric &gt;= success_threshold)\n\n\n#Task 4\n#1 What was the genre with the most “successes” in each decade?\n\n# Split genres into separate rows since some movies have multiple genres\nlibrary(tidyr)\n\nsuccessful_by_genre_decade &lt;- successful_titles |&gt;\n    separate_rows(genres, sep = \",\") |&gt;\n    group_by(decade, genres) |&gt;\n    summarize(num_successes = n(), .groups = 'drop')\n\n#1 Find the genre with the most successes in each decade\ntop_genre_by_decade &lt;- successful_by_genre_decade |&gt;\n    group_by(decade) |&gt;\n    slice_max(num_successes, n = 1)\n\ntop_genre_by_decade\n\n# A tibble: 12 × 3\n# Groups:   decade [12]\n   decade genres num_successes\n    &lt;dbl&gt; &lt;chr&gt;          &lt;int&gt;\n 1   1910 Drama             13\n 2   1920 Drama             96\n 3   1930 Drama            266\n 4   1940 Drama            420\n 5   1950 Drama            609\n 6   1960 Drama            806\n 7   1970 Drama            902\n 8   1980 Drama           1096\n 9   1990 Drama           1846\n10   2000 Drama           3182\n11   2010 Drama            435\n12   2020 Drama            755\n\n\n\n#Task 4\n# Plot: Genre with the most successes by decade\nggplot(top_genre_by_decade, aes(x = factor(decade), y = num_successes, fill = genres)) +\n    geom_col() +\n    labs(title = \"Top Genre with Most Successes by Decade\",\n         x = \"Decade\", y = \"Number of Successes\",\n         fill = \"Genre\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n#2 What genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\n\n# Step 1: Summarize successes by genre across all decades\ntotal_success_by_genre_decade &lt;- successful_titles |&gt;\n    separate_rows(genres, sep = \",\") |&gt;\n    group_by(decade, genres) |&gt;\n    summarize(num_successes = n(), .groups = 'drop')\n\n# Step 2: Find the genre with the most successes in each decade\ntop_genre_by_decade &lt;- total_success_by_genre_decade |&gt;\n    group_by(decade) |&gt;\n    slice_max(num_successes, n = 1, with_ties = FALSE) |&gt;\n    ungroup()\n\n# Step 3: Summarize the frequency of how often each genre is the top genre across decades\nconsistent_top_genres &lt;- top_genre_by_decade |&gt;\n    group_by(genres) |&gt;\n    summarize(times_top_genre = n(), .groups = 'drop') |&gt;\n    arrange(desc(times_top_genre))\n\n# View the result\nconsistent_top_genres\n\n# A tibble: 1 × 2\n  genres times_top_genre\n  &lt;chr&gt;            &lt;int&gt;\n1 Drama               12\n\n# Step 4: Compare successes in earlier decades to recent decades (since 2010)\n\n# Create a flag to distinguish early and recent decades\nsuccessful_by_genre_early_recent &lt;- total_success_by_genre_decade |&gt;\n    mutate(period = ifelse(decade &lt; 2010, \"Early Decades\", \"Recent Decades\"))\n\n# Summarize successes by genre in early and recent periods\nsuccess_by_genre_period &lt;- successful_by_genre_early_recent |&gt;\n    group_by(genres, period) |&gt;\n    summarize(total_successes = sum(num_successes), .groups = 'drop')\n\n# Spread to make it easier to compare early vs. recent\nlibrary(tidyr)\nsuccess_by_genre_period &lt;- success_by_genre_period |&gt;\n    pivot_wider(names_from = period, values_from = total_successes, values_fill = 0)\n\n# Calculate the difference in success between early and recent periods\nsuccess_by_genre_period &lt;- success_by_genre_period |&gt;\n    mutate(change_in_success = `Recent Decades` - `Early Decades`) |&gt;\n    arrange(change_in_success)\n\n# Genres that have fallen out of favor (negative change in success)\nfallen_genres &lt;- success_by_genre_period |&gt;\n    filter(change_in_success &lt; 0)\n\n# View genres that have fallen out of favor\nfallen_genres\n\n# A tibble: 28 × 4\n   genres    `Early Decades` `Recent Decades` change_in_success\n   &lt;chr&gt;               &lt;int&gt;            &lt;int&gt;             &lt;int&gt;\n 1 Drama                9236             1190             -8046\n 2 Comedy               5956              612             -5344\n 3 Romance              2894              266             -2628\n 4 Crime                2886              386             -2500\n 5 Action               2642              470             -2172\n 6 Adventure            2164              296             -1868\n 7 Thriller             1451              275             -1176\n 8 Mystery              1238              212             -1026\n 9 Family               1006               71              -935\n10 Animation            1069              147              -922\n# ℹ 18 more rows\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Assuming successful_by_genre_early_recent is already created as per your code\n\n# Create a flag to distinguish early and recent decades\nsuccessful_by_genre_early_recent &lt;- total_success_by_genre_decade |&gt;\n    mutate(period = ifelse(decade &lt; 2010, \"Early Decades\", \"Recent Decades\"))\n\n# Summarize successes by genre in early and recent periods\nsuccess_by_genre_period &lt;- successful_by_genre_early_recent |&gt;\n    group_by(genres, period) |&gt;\n    summarize(total_successes = sum(num_successes), .groups = 'drop')\n\n# Spread to make it easier to compare early vs. recent\nsuccess_by_genre_period &lt;- success_by_genre_period |&gt;\n    pivot_wider(names_from = period, values_from = total_successes, values_fill = 0)\n\n# Create a line plot to visualize successes over periods\nggplot(success_by_genre_period, aes(x = genres)) +\n    geom_line(aes(y = `Early Decades`, color = \"Early Decades\"), size = 1) +\n    geom_line(aes(y = `Recent Decades`, color = \"Recent Decades\"), size = 1) +\n    geom_point(aes(y = `Early Decades`, color = \"Early Decades\"), size = 3) +\n    geom_point(aes(y = `Recent Decades`, color = \"Recent Decades\"), size = 3) +\n    labs(title = \"Total Successes by Genre: Early vs. Recent Decades\",\n         x = \"Genres\",\n         y = \"Total Successes\",\n         color = \"Period\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\n\n#Task 4 \n#3 What genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\n\n# Step 1: Filter for titles since 2010\ntitles_since_2010 &lt;- successful_titles |&gt;\n    filter(decade &gt;= 2010)\n\n# Step 2: Separate rows by genre for proper counting\ntitles_genre_split &lt;- titles_since_2010 |&gt;\n    separate_rows(genres, sep = \",\")\n\n# Step 3: Count total titles produced in each genre since 2010\ntotal_titles_by_genre &lt;- TITLE_BASICS |&gt;\n    filter(startYear &gt;= 2010) |&gt;\n    separate_rows(genres, sep = \",\") |&gt;\n    group_by(genres) |&gt;\n    summarize(total_titles = n(), .groups = 'drop')\n\n# Step 4: Count successful titles (based on your custom success metric) per genre since 2010\nsuccessful_titles_by_genre &lt;- titles_genre_split |&gt;\n    group_by(genres) |&gt;\n    summarize(successful_titles = n(), .groups = 'drop')\n\n# Step 5: Merge the total titles and successful titles data\ngenre_success_data &lt;- total_titles_by_genre |&gt;\n    left_join(successful_titles_by_genre, by = \"genres\") |&gt;\n    replace_na(list(successful_titles = 0)) # If no successes, set to 0\n\n# Step 6: Calculate the success rate for each genre\ngenre_success_data &lt;- genre_success_data |&gt;\n    mutate(success_rate = successful_titles / total_titles) |&gt;\n    arrange(desc(successful_titles))\n\n# View the top genres by number of successes\nhead(genre_success_data, 10)\n\n# A tibble: 10 × 4\n   genres      total_titles successful_titles success_rate\n   &lt;chr&gt;              &lt;int&gt;             &lt;int&gt;        &lt;dbl&gt;\n 1 Drama              10990              1190       0.108 \n 2 Comedy              6414               612       0.0954\n 3 Action              4446               470       0.106 \n 4 Crime               3687               386       0.105 \n 5 Adventure           3200               296       0.0925\n 6 Thriller            2322               275       0.118 \n 7 Romance             2165               266       0.123 \n 8 Documentary         2205               221       0.100 \n 9 Mystery             2061               212       0.103 \n10 Biography            690               171       0.248 \n\n\n\n#Task4\n#4 What genre has become more popular in recent years?\n# Step 1: Separate genres into individual rows and calculate success by genre and decade\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Separate genres and group by decade and genre\nsuccess_by_genre_decade &lt;- successful_titles |&gt;\n    separate_rows(genres, sep = \",\") |&gt;\n    group_by(decade, genres) |&gt;\n    summarize(num_successes = n(), .groups = 'drop')\n\n# Step 2: Categorize periods into \"Early Decades\" and \"Recent Decades\" (2010 onwards)\nsuccess_by_genre_period &lt;- success_by_genre_decade |&gt;\n    mutate(period = ifelse(decade &lt; 2010, \"Early Decades\", \"Recent Decades\"))\n\n# Step 3: Summarize the total successes by genre in early vs recent periods\nsuccess_by_genre_period_summary &lt;- success_by_genre_period |&gt;\n    group_by(genres, period) |&gt;\n    summarize(total_successes = sum(num_successes), .groups = 'drop')\n\n# Step 4: Reshape the data to compare success between periods (early vs recent)\nsuccess_by_genre_comparison &lt;- success_by_genre_period_summary |&gt;\n    pivot_wider(names_from = period, values_from = total_successes, values_fill = 0)\n\n# Step 5: Calculate the change in success between early and recent periods\nsuccess_by_genre_comparison &lt;- success_by_genre_comparison |&gt;\n    mutate(change_in_success = `Recent Decades` - `Early Decades`) |&gt;\n    arrange(desc(change_in_success))\n\n# Step 6: Identify genres that have become more popular in recent years (positive change)\npopular_genres_recent_years &lt;- success_by_genre_comparison |&gt;\n    filter(change_in_success &gt; 0)\n\n# View the genres that have become more popular\npopular_genres_recent_years\n\n# A tibble: 1 × 4\n  genres `Early Decades` `Recent Decades` change_in_success\n  &lt;chr&gt;            &lt;int&gt;            &lt;int&gt;             &lt;int&gt;\n1 Short                5                6                 1\n\n\n#Task 5: Movie Pitch # Define the pitch text pitch &lt;- ” Title: Shadows in the Horizon\nPlot Overview: In a dystopian future where artificial intelligence has gained near-total control over human society, a group of unlikely heroes comes together to dismantle the system that has suppressed freedom for decades. Their journey takes them through moral dilemmas, high-stakes battles, and the ultimate realization that the enemy may not be as simple as it seems.\nWhy This Team? Michael B. Jordan is known for his incredible performances in films like Creed and Black Panther. He brings intensity and emotional depth to the lead role, embodying both strength and vulnerability, making him the perfect choice to lead a rebellion against an all-powerful system. Scarlett Johansson, with her iconic performances in Lucy and the Avengers series, adds charisma and complexity to the role of a former AI scientist who questions her past decisions. Her experience in action-packed, sci-fi roles makes her an invaluable part of the cast. Denis Villeneuve, the visionary director behind Blade Runner 2049 and Dune, is ideal for bringing this dark, futuristic world to life. His exceptional ability to combine stunning visuals with intricate storytelling will elevate Shadows in the Horizon beyond a typical sci-fi film. # Print paragraph\nSupporting Data: We have identified key personnel whose past projects in the action/sci-fi genres have been immensely successful, as shown by both IMDb ratings and our custom success metric. Below is a graphical representation of their success: ”\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Sample data for past successful projects\npersonnel_success &lt;- data.frame(\n  name = c(\"Michael B. Jordan\", \"Michael B. Jordan\", \"Scarlett Johansson\", \"Scarlett Johansson\", \"Denis Villeneuve\", \"Denis Villeneuve\"),\n  movie = c(\"Black Panther\", \"Creed\", \"Lucy\", \"Avengers: Endgame\", \"Blade Runner 2049\", \"Dune\"),\n  imdb_rating = c(7.3, 7.6, 6.4, 8.4, 8.0, 8.2),\n  success_metric = c(8.1, 7.9, 6.5, 9.0, 8.4, 8.5)\n)\n\n# Create the graph\nggplot(personnel_success, aes(x = movie, y = success_metric, fill = name)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  geom_text(aes(label = round(success_metric, 1)), vjust = -0.5, color = \"black\") +\n  labs(title = \"Success Metrics of Key Personnel's Past Projects\",\n       x = \"Movie\",\n       y = \"Success Metric\",\n       fill = \"Personnel\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n# Table for key personnel's past successful projects\ntable_data &lt;- personnel_success %&gt;%\n  select(name, movie, imdb_rating, success_metric) %&gt;%\n  arrange(desc(success_metric))\n\n# Display table as output\nlibrary(knitr)\nkable(table_data, col.names = c(\"Name\", \"Movie\", \"IMDb Rating\", \"Success Metric\"),\n      caption = \"IMDb Ratings and Success Metrics of Key Personnel's Past Projects\")\n\n\nIMDb Ratings and Success Metrics of Key Personnel’s Past Projects\n\n\nName\nMovie\nIMDb Rating\nSuccess Metric\n\n\n\n\nScarlett Johansson\nAvengers: Endgame\n8.4\n9.0\n\n\nDenis Villeneuve\nDune\n8.2\n8.5\n\n\nDenis Villeneuve\nBlade Runner 2049\n8.0\n8.4\n\n\nMichael B. Jordan\nBlack Panther\n7.3\n8.1\n\n\nMichael B. Jordan\nCreed\n7.6\n7.9\n\n\nScarlett Johansson\nLucy\n6.4\n6.5\n\n\n\n\n\n\n#Task 6\n#Finding a Classic movie\n# Assuming TITLE_BASICS contains the movie data and TITLE_RATINGS has the ratings data\nlibrary(dplyr)\n\n# Filtering for sci-fi/action movies with high IMDb rating and vote count, not remade recently\nclassic_movies &lt;- TITLE_BASICS %&gt;%\n  # Join with ratings data to get the IMDb rating and number of votes\n  inner_join(TITLE_RATINGS, by = \"tconst\") %&gt;%\n  # Filter by genres containing 'Sci-Fi' or 'Action'\n  filter(grepl(\"Sci-Fi|Action\", genres)) %&gt;%\n  # Filter for movies with a release date before 1999 (not remade in 25 years)\n  filter(startYear &lt; 1999) %&gt;%\n  # Filter for movies with a high average rating and large number of votes\n  filter(averageRating &gt;= 7.5, numVotes &gt; 50000) %&gt;%\n  arrange(desc(averageRating), desc(numVotes)) %&gt;%\n  select(primaryTitle, startYear, genres, averageRating, numVotes)\n\n# Display top 5 classic movies\nhead(classic_movies, 5)\n\n                                    primaryTitle startYear\n1                    Batman: The Animated Series      1992\n2                                   Cowboy Bebop      1998\n3                                  Dragon Ball Z      1996\n4                                  Dragon Ball Z      1989\n5 Star Wars: Episode V - The Empire Strikes Back      1980\n                      genres averageRating numVotes\n1 Action,Adventure,Animation           9.0   121064\n2 Action,Adventure,Animation           8.9   144704\n3 Action,Adventure,Animation           8.8   153003\n4 Action,Adventure,Animation           8.8    90267\n5   Action,Adventure,Fantasy           8.7  1405462\n\n\n\n#LETS CHOOSE!: Star Wars: Episode V - The Empire Strikes Back\n# Assuming TITLE_BASICS has the movie data, TITLE_PRINCIPALS contains personnel data, and NAME_BASICS has names and birth/death years\n\nlibrary(dplyr)\n\n# Get the tconst for \"Star Wars: Episode V - The Empire Strikes Back\"\nstar_wars_tconst &lt;- TITLE_BASICS %&gt;%\n  filter(primaryTitle == \"Star Wars: Episode V - The Empire Strikes Back\" & startYear == 1980) %&gt;%\n  select(tconst)\n\n# Check if any key personnel (actors, directors, writers) from the movie are still alive\nstar_wars_personnel &lt;- TITLE_PRINCIPALS %&gt;%\n  filter(tconst == star_wars_tconst$tconst) %&gt;%\n  inner_join(NAME_BASICS, by = \"nconst\") %&gt;%\n  filter(category %in% c(\"actor\", \"director\", \"writer\")) %&gt;%\n  select(primaryName, birthYear, deathYear, category)\n\n# Filter to check if they are still alive (deathYear is NA)\nliving_star_wars_personnel &lt;- star_wars_personnel %&gt;%\n  filter(is.na(deathYear))\n\n# Display living key personnel\nliving_star_wars_personnel\n\n         primaryName birthYear deathYear category\n1        Mark Hamill        NA        NA    actor\n2      Harrison Ford      1942        NA    actor\n3 Billy Dee Williams      1937        NA    actor\n4    Anthony Daniels      1946        NA    actor\n5           Frank Oz      1944        NA    actor\n6    Lawrence Kasdan      1949        NA   writer\n7       George Lucas      1944        NA   writer\n\n\n\nCreate a short paragraph explaining the casting choice and legal department contact\nparagraph &lt;- ” We have successfully contacted the legal departments to secure the rights to remake the iconic ‘Star Wars: Episode V - The Empire Strikes Back’. This classic film will be re-envisioned with the visionary director Denis Villeneuve, whose expertise in handling large-scale sci-fi epics will make the perfect fit for a remake. With Michael B. Jordan as the charismatic and action-driven lead, we believe he will bring a fresh, modern interpretation to the role of a young Jedi. Scarlett Johansson, with her versatility and experience in sci-fi blockbusters, will add depth to the character of Princess Leia, providing both strength and emotional gravity. This combination of talent and creative direction ensures that the remake will honor the original’s legacy while appealing to both long-time fans and new audiences alike. ”\n#Task 7 # Elevator pitch paragraph pitch &lt;- ”\nAfter a very analysis of market trends and key personnel, I am confident that our proposed remake of ‘Star Wars: Episode V - The Empire Strikes Back’ will be a major success. First, the sci-fi genre continues to thrive in our time, with massive box office successes driven by cutting-edge visual effects and nostalgic storytelling, capturing the hearts of viewers of all ages. Denis Villeneuve, known for his visionary work in ‘Blade Runner 2049’ and ‘Dune,’ has a proven track record in crafting visually stunning and deeply engaging sci-fi epics. His directorial style aligns perfectly with the scale and depth required to reimagine the Star Wars universe. Secondly, we have secured tentative interest from two A-list actors. Michael B. Jordan, a rising star in action and adventure films, will bring new energy to the role of the young Jedi. Scarlett Johansson, a versatile actress with numerous successful sci-fi credits, will redefine the role of Princess Leia, bringing emotional depth and charisma to the screen. Lastly, ‘Star Wars: Episode V’ remains an iconic film with a massive fanbase. A modern, high-quality remake has the potential to capture both new audiences and long-time fans, making this project not only artistically exciting but commercially promising. Let’s greenlight this project and bring this timeless tale back to life! From Denis Villeneuve, the visionary mind behind ‘Dune’ and ‘Blade Runner 2049’; and From Michael B. Jordan, beloved star of ‘Creed’ and ‘Black Panther’; and From Scarlett Johansson, Hollywood icon of sci-fi hits like ‘Lucy’ and ‘The Avengers’; Comes the timeless tale of ‘Star Wars: Episode V - The Empire Strikes Back.’\nA story of rebellion, hope, and destiny, coming soon to a theater near you.\nWith Michael B. Jordan set to bring new energy to the role of a young Jedi and Scarlett Johansson redefining the iconic Princess Leia, this remake promises to captivate both longtime fans and new audiences. Let’s greenlight this project and bring this timeless tale back to life!”"
  },
  {
    "objectID": "mp04.html#task-1",
    "href": "mp04.html#task-1",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 1",
    "text": "Task 1\nFor this first task all we are doing is registering for an AlphaVantage API Key in order to access documentation and data that we will need for for our project.\n\n\nClick to Show/Hide Code\n# Specify the path to the file containing your API key\napi_key_path &lt;- \"alphavantage_key.txt\"\n\n# Read the API key securely\nalpha_vantage_key &lt;- readLines(api_key_path)[1]\n\n\n\n\nClick to Show/Hide Code\nif (!is.null(alpha_vantage_key)) {\n    message(\"API key successfully read.\")\n} else {\n    stop(\"Failed to read API key.\")\n}\n\n\nAPI key successfully read."
  },
  {
    "objectID": "mp04.html#task-2",
    "href": "mp04.html#task-2",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 2",
    "text": "Task 2\nFor this task, similar to the first one all we are doing is registering for a FRED API Key to access data and documentation we will need to conduct this mini project.\n\n\nClick to Show/Hide Code\n# Specify the path to the file containing your API key\napi_key_path &lt;- \"fred_key.txt\"\n\n# Read the API key securely\nfred_api_key &lt;- readLines(api_key_path)[1]\n\n\n\n\nClick to Show/Hide Code\nif (!is.null(fred_api_key)) {\n    message(\"FRED API key successfully read.\")\n} else {\n    stop(\"Failed to read FRED API key.\")\n}\n\n\nFRED API key successfully read."
  },
  {
    "objectID": "mp04.html#task-3",
    "href": "mp04.html#task-3",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 3",
    "text": "Task 3\nFor this task, we will now begin our Monte Carlo Analysis. In order to do this we will utilize the API keys we previously secured to access historical data from both data sources, AlphaVantage and FRED. The historical data series that we are looking to download are from inputs such as inflation, wage growth, US Equity Market total returns, International Equity Market total returns, bond market total returns, and short-term debt returns.\n\n\nClick to Show/Hide Code\nlibrary(httr2)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nClick to Show/Hide Code\nlibrary(tidyr)\nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nClick to Show/Hide Code\nlibrary(purrr)  # For map_chr()\n\n# Load API keys\nalpha_key &lt;- readLines(\"alphavantage_key.txt\")\nfred_key &lt;- readLines(\"fred_key.txt\")\n\n# Define base URLs\nalpha_base_url &lt;- \"https://www.alphavantage.co/query\"\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# Function to fetch data from AlphaVantage\nfetch_alpha_data &lt;- function(symbol, function_type = \"TIME_SERIES_MONTHLY\") {\n  req &lt;- request(alpha_base_url) |&gt;\n    req_url_query(\n      `function` = function_type,\n      symbol = symbol,\n      apikey = alpha_key\n    )\n  \n  # Perform the request\n  response &lt;- req |&gt; req_perform() |&gt; resp_body_json()\n  \n  # Parse the response to extract time series data\n  time_series &lt;- response[[paste0(\"Monthly Time Series\")]]\n  \n  # Convert time series into a tibble\n  tibble(\n    date = as.Date(names(time_series)),\n    adjusted_close = sapply(time_series, function(x) as.numeric(x[[\"5. adjusted close\"]]))\n  )\n}\n\n# Function to fetch data from FRED\nfetch_fred_data &lt;- function(series_id) {\n  req &lt;- request(fred_base_url) |&gt;\n    req_url_query(\n      series_id = series_id,\n      api_key = fred_key,\n      file_type = \"json\"\n    )\n  \n  # Perform the request\n  response &lt;- req |&gt; req_perform() |&gt; resp_body_json()\n  \n  # Parse the response to extract observations\n  observations &lt;- response[[\"observations\"]]\n  \n  # Convert observations into a tibble\n  tibble(\n    date = as.Date(sapply(observations, `[[`, \"date\")),\n    value = as.numeric(sapply(observations, `[[`, \"value\"))\n  )\n}\n\n# Fetch required data from AlphaVantage and FRED\nus_equity &lt;- fetch_alpha_data(\"SPY\")           # US Equity Market Proxy\nintl_equity &lt;- fetch_alpha_data(\"EFA\")         # International Equity Market Proxy\nwage_growth &lt;- fetch_fred_data(\"CES0500000003\") # Wage Growth Proxy\ninflation &lt;- fetch_fred_data(\"CPIAUCSL\")        # Inflation Proxy\nbond_returns &lt;- fetch_fred_data(\"AAA\")         # Bond Returns Proxy\nshort_term_debt &lt;- fetch_fred_data(\"TB3MS\")    # Short-Term Debt Returns Proxy\n\n# Ensure data is aggregated to monthly frequency\nus_equity &lt;- us_equity |&gt; mutate(date = floor_date(date, \"month\"))\nintl_equity &lt;- intl_equity |&gt; mutate(date = floor_date(date, \"month\"))\nwage_growth &lt;- wage_growth |&gt; mutate(date = floor_date(date, \"month\"))\ninflation &lt;- inflation |&gt; mutate(date = floor_date(date, \"month\"))\nbond_returns &lt;- bond_returns |&gt; mutate(date = floor_date(date, \"month\"))\nshort_term_debt &lt;- short_term_debt |&gt; mutate(date = floor_date(date, \"month\"))\n\n# Combine all datasets into a single data frame\nfinal_data &lt;- full_join(us_equity, intl_equity, by = \"date\", suffix = c(\"_us\", \"_intl\")) |&gt;\n  full_join(wage_growth, by = \"date\", suffix = c(\"\", \"_wage\")) |&gt;\n  full_join(inflation, by = \"date\", suffix = c(\"\", \"_inflation\")) |&gt;\n  full_join(bond_returns, by = \"date\", suffix = c(\"\", \"_bond\")) |&gt;\n  full_join(short_term_debt, by = \"date\", suffix = c(\"\", \"_short_term\"))\n\n# Debugging: Inspect structure of the combined data\nprint(str(final_data))\n\n\ntibble [1,271 × 7] (S3: tbl_df/tbl/data.frame)\n $ date               : Date[1:1271], format: \"2024-11-01\" \"2024-10-01\" ...\n $ adjusted_close_us  :List of 1271\n  ..$ 2024-11-29: num(0) \n  ..$ 2024-10-31: num(0) \n  ..$ 2024-09-30: num(0) \n  ..$ 2024-08-30: num(0) \n  ..$ 2024-07-31: num(0) \n  ..$ 2024-06-28: num(0) \n  ..$ 2024-05-31: num(0) \n  ..$ 2024-04-30: num(0) \n  ..$ 2024-03-28: num(0) \n  ..$ 2024-02-29: num(0) \n  ..$ 2024-01-31: num(0) \n  ..$ 2023-12-29: num(0) \n  ..$ 2023-11-30: num(0) \n  ..$ 2023-10-31: num(0) \n  ..$ 2023-09-29: num(0) \n  ..$ 2023-08-31: num(0) \n  ..$ 2023-07-31: num(0) \n  ..$ 2023-06-30: num(0) \n  ..$ 2023-05-31: num(0) \n  ..$ 2023-04-28: num(0) \n  ..$ 2023-03-31: num(0) \n  ..$ 2023-02-28: num(0) \n  ..$ 2023-01-31: num(0) \n  ..$ 2022-12-30: num(0) \n  ..$ 2022-11-30: num(0) \n  ..$ 2022-10-31: num(0) \n  ..$ 2022-09-30: num(0) \n  ..$ 2022-08-31: num(0) \n  ..$ 2022-07-29: num(0) \n  ..$ 2022-06-30: num(0) \n  ..$ 2022-05-31: num(0) \n  ..$ 2022-04-29: num(0) \n  ..$ 2022-03-31: num(0) \n  ..$ 2022-02-28: num(0) \n  ..$ 2022-01-31: num(0) \n  ..$ 2021-12-31: num(0) \n  ..$ 2021-11-30: num(0) \n  ..$ 2021-10-29: num(0) \n  ..$ 2021-09-30: num(0) \n  ..$ 2021-08-31: num(0) \n  ..$ 2021-07-30: num(0) \n  ..$ 2021-06-30: num(0) \n  ..$ 2021-05-28: num(0) \n  ..$ 2021-04-30: num(0) \n  ..$ 2021-03-31: num(0) \n  ..$ 2021-02-26: num(0) \n  ..$ 2021-01-29: num(0) \n  ..$ 2020-12-31: num(0) \n  ..$ 2020-11-30: num(0) \n  ..$ 2020-10-30: num(0) \n  ..$ 2020-09-30: num(0) \n  ..$ 2020-08-31: num(0) \n  ..$ 2020-07-31: num(0) \n  ..$ 2020-06-30: num(0) \n  ..$ 2020-05-29: num(0) \n  ..$ 2020-04-30: num(0) \n  ..$ 2020-03-31: num(0) \n  ..$ 2020-02-28: num(0) \n  ..$ 2020-01-31: num(0) \n  ..$ 2019-12-31: num(0) \n  ..$ 2019-11-29: num(0) \n  ..$ 2019-10-31: num(0) \n  ..$ 2019-09-30: num(0) \n  ..$ 2019-08-30: num(0) \n  ..$ 2019-07-31: num(0) \n  ..$ 2019-06-28: num(0) \n  ..$ 2019-05-31: num(0) \n  ..$ 2019-04-30: num(0) \n  ..$ 2019-03-29: num(0) \n  ..$ 2019-02-28: num(0) \n  ..$ 2019-01-31: num(0) \n  ..$ 2018-12-31: num(0) \n  ..$ 2018-11-30: num(0) \n  ..$ 2018-10-31: num(0) \n  ..$ 2018-09-28: num(0) \n  ..$ 2018-08-31: num(0) \n  ..$ 2018-07-31: num(0) \n  ..$ 2018-06-29: num(0) \n  ..$ 2018-05-31: num(0) \n  ..$ 2018-04-30: num(0) \n  ..$ 2018-03-29: num(0) \n  ..$ 2018-02-28: num(0) \n  ..$ 2018-01-31: num(0) \n  ..$ 2017-12-29: num(0) \n  ..$ 2017-11-30: num(0) \n  ..$ 2017-10-31: num(0) \n  ..$ 2017-09-29: num(0) \n  ..$ 2017-08-31: num(0) \n  ..$ 2017-07-31: num(0) \n  ..$ 2017-06-30: num(0) \n  ..$ 2017-05-31: num(0) \n  ..$ 2017-04-28: num(0) \n  ..$ 2017-03-31: num(0) \n  ..$ 2017-02-28: num(0) \n  ..$ 2017-01-31: num(0) \n  ..$ 2016-12-30: num(0) \n  ..$ 2016-11-30: num(0) \n  ..$ 2016-10-31: num(0) \n  ..$ 2016-09-30: num(0) \n  .. [list output truncated]\n $ adjusted_close_intl:List of 1271\n  ..$ 2024-11-29: num(0) \n  ..$ 2024-10-31: num(0) \n  ..$ 2024-09-30: num(0) \n  ..$ 2024-08-30: num(0) \n  ..$ 2024-07-31: num(0) \n  ..$ 2024-06-28: num(0) \n  ..$ 2024-05-31: num(0) \n  ..$ 2024-04-30: num(0) \n  ..$ 2024-03-28: num(0) \n  ..$ 2024-02-29: num(0) \n  ..$ 2024-01-31: num(0) \n  ..$ 2023-12-29: num(0) \n  ..$ 2023-11-30: num(0) \n  ..$ 2023-10-31: num(0) \n  ..$ 2023-09-29: num(0) \n  ..$ 2023-08-31: num(0) \n  ..$ 2023-07-31: num(0) \n  ..$ 2023-06-30: num(0) \n  ..$ 2023-05-31: num(0) \n  ..$ 2023-04-28: num(0) \n  ..$ 2023-03-31: num(0) \n  ..$ 2023-02-28: num(0) \n  ..$ 2023-01-31: num(0) \n  ..$ 2022-12-30: num(0) \n  ..$ 2022-11-30: num(0) \n  ..$ 2022-10-31: num(0) \n  ..$ 2022-09-30: num(0) \n  ..$ 2022-08-31: num(0) \n  ..$ 2022-07-29: num(0) \n  ..$ 2022-06-30: num(0) \n  ..$ 2022-05-31: num(0) \n  ..$ 2022-04-29: num(0) \n  ..$ 2022-03-31: num(0) \n  ..$ 2022-02-28: num(0) \n  ..$ 2022-01-31: num(0) \n  ..$ 2021-12-31: num(0) \n  ..$ 2021-11-30: num(0) \n  ..$ 2021-10-29: num(0) \n  ..$ 2021-09-30: num(0) \n  ..$ 2021-08-31: num(0) \n  ..$ 2021-07-30: num(0) \n  ..$ 2021-06-30: num(0) \n  ..$ 2021-05-28: num(0) \n  ..$ 2021-04-30: num(0) \n  ..$ 2021-03-31: num(0) \n  ..$ 2021-02-26: num(0) \n  ..$ 2021-01-29: num(0) \n  ..$ 2020-12-31: num(0) \n  ..$ 2020-11-30: num(0) \n  ..$ 2020-10-30: num(0) \n  ..$ 2020-09-30: num(0) \n  ..$ 2020-08-31: num(0) \n  ..$ 2020-07-31: num(0) \n  ..$ 2020-06-30: num(0) \n  ..$ 2020-05-29: num(0) \n  ..$ 2020-04-30: num(0) \n  ..$ 2020-03-31: num(0) \n  ..$ 2020-02-28: num(0) \n  ..$ 2020-01-31: num(0) \n  ..$ 2019-12-31: num(0) \n  ..$ 2019-11-29: num(0) \n  ..$ 2019-10-31: num(0) \n  ..$ 2019-09-30: num(0) \n  ..$ 2019-08-30: num(0) \n  ..$ 2019-07-31: num(0) \n  ..$ 2019-06-28: num(0) \n  ..$ 2019-05-31: num(0) \n  ..$ 2019-04-30: num(0) \n  ..$ 2019-03-29: num(0) \n  ..$ 2019-02-28: num(0) \n  ..$ 2019-01-31: num(0) \n  ..$ 2018-12-31: num(0) \n  ..$ 2018-11-30: num(0) \n  ..$ 2018-10-31: num(0) \n  ..$ 2018-09-28: num(0) \n  ..$ 2018-08-31: num(0) \n  ..$ 2018-07-31: num(0) \n  ..$ 2018-06-29: num(0) \n  ..$ 2018-05-31: num(0) \n  ..$ 2018-04-30: num(0) \n  ..$ 2018-03-29: num(0) \n  ..$ 2018-02-28: num(0) \n  ..$ 2018-01-31: num(0) \n  ..$ 2017-12-29: num(0) \n  ..$ 2017-11-30: num(0) \n  ..$ 2017-10-31: num(0) \n  ..$ 2017-09-29: num(0) \n  ..$ 2017-08-31: num(0) \n  ..$ 2017-07-31: num(0) \n  ..$ 2017-06-30: num(0) \n  ..$ 2017-05-31: num(0) \n  ..$ 2017-04-28: num(0) \n  ..$ 2017-03-31: num(0) \n  ..$ 2017-02-28: num(0) \n  ..$ 2017-01-31: num(0) \n  ..$ 2016-12-30: num(0) \n  ..$ 2016-11-30: num(0) \n  ..$ 2016-10-31: num(0) \n  ..$ 2016-09-30: num(0) \n  .. [list output truncated]\n $ value              : num [1:1271] NA 35.5 35.3 35.2 35.1 ...\n $ value_inflation    : num [1:1271] NA 315 315 314 314 ...\n $ value_bond         : num [1:1271] NA 4.95 4.68 4.87 5.12 5.13 5.25 5.28 5.01 5.03 ...\n $ value_short_term   : num [1:1271] NA 4.51 4.72 5.05 5.2 5.24 5.25 5.24 5.24 5.24 ...\nNULL\n\n\nClick to Show/Hide Code\n# Convert any list columns into atomic vectors explicitly\nfinal_data &lt;- final_data |&gt;\n  mutate(across(where(is.list), ~ map_chr(., ~ ifelse(is.null(.), NA, as.character(.)))))\n\n# Verify structure again after flattening\nprint(str(final_data))\n\n\ntibble [1,271 × 7] (S3: tbl_df/tbl/data.frame)\n $ date               : Date[1:1271], format: \"2024-11-01\" \"2024-10-01\" ...\n $ adjusted_close_us  : Named chr [1:1271] NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:1271] \"2024-11-29\" \"2024-10-31\" \"2024-09-30\" \"2024-08-30\" ...\n $ adjusted_close_intl: Named chr [1:1271] NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:1271] \"2024-11-29\" \"2024-10-31\" \"2024-09-30\" \"2024-08-30\" ...\n $ value              : num [1:1271] NA 35.5 35.3 35.2 35.1 ...\n $ value_inflation    : num [1:1271] NA 315 315 314 314 ...\n $ value_bond         : num [1:1271] NA 4.95 4.68 4.87 5.12 5.13 5.25 5.28 5.01 5.03 ...\n $ value_short_term   : num [1:1271] NA 4.51 4.72 5.05 5.2 5.24 5.25 5.24 5.24 5.24 ...\nNULL\n\n\nClick to Show/Hide Code\n# Rename columns for clarity\nfinal_data &lt;- final_data |&gt;\n  rename(\n    wage_growth = value,                   # Wage growth column correctly renamed\n    inflation = value_inflation,\n    bond_returns = value_bond,\n    short_term_debt_returns = value_short_term\n  )\n\n# Handle missing values (if any)\nfinal_data &lt;- final_data |&gt; drop_na()\n\n# Save the final dataset to a CSV file\nwrite.csv(final_data, \"historical_economic_data_with_bonds.csv\", row.names = FALSE)"
  },
  {
    "objectID": "mp04.html#task-1-register-for-alphavantage-api-key",
    "href": "mp04.html#task-1-register-for-alphavantage-api-key",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 1 Register for AlphaVantage API Key",
    "text": "Task 1 Register for AlphaVantage API Key\nFor this first task all we are doing is registering for an AlphaVantage API Key in order to access documentation and data that we will need for for our project.\n\n\nClick to Show/Hide Code\n# Specify the path to the file containing your API key\napi_key_path &lt;- \"alphavantage_key.txt\"\n\n# Read the API key securely\nalpha_vantage_key &lt;- readLines(api_key_path)[1]\n\n\n\n\nClick to Show/Hide Code\nif (!is.null(alpha_vantage_key)) {\n    message(\"API key successfully read.\")\n} else {\n    stop(\"Failed to read API key.\")\n}\n\n\nAPI key successfully read."
  },
  {
    "objectID": "mp04.html#task-2-register-for-fred-api-key",
    "href": "mp04.html#task-2-register-for-fred-api-key",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 2 Register for FRED API Key",
    "text": "Task 2 Register for FRED API Key\nFor this task, similar to the first one all we are doing is registering for a FRED API Key to access data and documentation we will need to conduct this mini project.\n\n\nClick to Show/Hide Code\n# Specify the path to the file containing your API key\napi_key_path &lt;- \"fred_key.txt\"\n\n# Read the API key securely\nfred_api_key &lt;- readLines(api_key_path)[1]\n\n\n\n\nClick to Show/Hide Code\nif (!is.null(fred_api_key)) {\n    message(\"FRED API key successfully read.\")\n} else {\n    stop(\"Failed to read FRED API key.\")\n}\n\n\nFRED API key successfully read."
  },
  {
    "objectID": "mp04.html#task-3-data-acquisition",
    "href": "mp04.html#task-3-data-acquisition",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 3 Data Acquisition",
    "text": "Task 3 Data Acquisition\nFor this task, we will now begin our Monte Carlo Analysis. In order to do this we will utilize the API keys we previously secured to access historical data from both data sources, AlphaVantage and FRED. The historical data series that we are looking to download are from inputs such as inflation, wage growth, US Equity Market total returns, International Equity Market total returns, bond market total returns, and short-term debt returns.\n\n\nClick to Show/Hide Code\nlibrary(httr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nClick to Show/Hide Code\nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nClick to Show/Hide Code\nlibrary(purrr)\nlibrary(tidyr)  # For fill()\nlibrary(DT)     # For scrollable tables\n\n# Load API keys\nalpha_key &lt;- readLines(\"alphavantage_key.txt\")\nfred_key &lt;- readLines(\"fred_key.txt\")\n\n# Base URLs\nalpha_base_url &lt;- \"https://www.alphavantage.co/query\"\nfred_base_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# Define the date range\nstart_date &lt;- floor_date(Sys.Date() - years(15), \"month\")\nend_date &lt;- floor_date(Sys.Date(), \"month\")\n\n# Function to fetch AlphaVantage data\nfetch_alpha_data &lt;- function(symbol) {\n  response &lt;- GET(\n    url = paste0(alpha_base_url, \n                 \"?function=TIME_SERIES_MONTHLY_ADJUSTED\",\n                 \"&symbol=\", symbol, \n                 \"&apikey=\", alpha_key)\n  )\n  content &lt;- content(response, \"parsed\")\n  time_series &lt;- content[[\"Monthly Adjusted Time Series\"]]\n  if (is.null(time_series)) stop(paste(\"No data returned for symbol:\", symbol))\n  \n  tibble(\n    date = as.Date(names(time_series)),\n    adjusted_close = map_dbl(time_series, ~ as.numeric(.x[[\"5. adjusted close\"]] %||% NA_real_))\n  ) |&gt; filter(date &gt;= start_date & date &lt;= end_date)\n}\n\n# Function to fetch FRED data\nfetch_fred_data &lt;- function(series_id) {\n  response &lt;- GET(\n    url = paste0(fred_base_url,\n                 \"?series_id=\", series_id,\n                 \"&api_key=\", fred_key,\n                 \"&file_type=json\")\n  )\n  content &lt;- content(response, \"parsed\")\n  observations &lt;- content[[\"observations\"]]\n  if (is.null(observations)) stop(paste(\"No data returned for FRED series:\", series_id))\n  \n  tibble(\n    date = as.Date(map_chr(observations, \"date\")),\n    value = map_dbl(observations, ~ as.numeric(.x[[\"value\"]]))  # Explicit conversion\n  ) |&gt; filter(date &gt;= start_date & date &lt;= end_date)\n}\n\n# Fetch datasets\nibm_data &lt;- fetch_alpha_data(\"IBM\")  # IBM as the US equity proxy\nintl_equity &lt;- fetch_alpha_data(\"EFA\")  # iShares MSCI EAFE ETF as international equity proxy\nwage_growth &lt;- fetch_fred_data(\"CES0500000003\")  # Wage growth\ninflation &lt;- fetch_fred_data(\"CPIAUCSL\")  # Inflation\nbond_returns &lt;- fetch_fred_data(\"AAA\")  # Bond returns\nshort_term_debt &lt;- fetch_fred_data(\"TB3MS\")  # Short-term debt\n\n# Combine datasets\nfinal_data &lt;- list(\n  us_equity = ibm_data |&gt; rename(adjusted_close_us = adjusted_close),\n  intl_equity = intl_equity |&gt; rename(adjusted_close_intl = adjusted_close),\n  wage_growth = wage_growth |&gt; rename(wage_growth = value),\n  inflation = inflation |&gt; rename(inflation = value),\n  bond_returns = bond_returns |&gt; rename(bond_returns = value),\n  short_term_debt = short_term_debt |&gt; rename(short_term_debt_returns = value)\n) |&gt; reduce(full_join, by = \"date\")  # Combine datasets by \"date\"\n\n# Fill missing values\nfinal_data &lt;- final_data |&gt; \n  arrange(date) |&gt;  # Sort by date\n  fill(everything(), .direction = \"downup\")  # Forward and backward fill\n\n# Save the cleaned and imputed dataset\nwrite.csv(final_data, \"historical_economic_data_full.csv\", row.names = FALSE)\nprint(\"Full data saved successfully to 'historical_economic_data_full.csv'.\")\n\n\n[1] \"Full data saved successfully to 'historical_economic_data_full.csv'.\"\n\n\nClick to Show/Hide Code\n# Display as a scrollable, subsampled table in RStudio\ndatatable(\n  final_data,\n  options = list(\n    scrollX = TRUE,  # Enable horizontal scrolling\n    scrollY = \"500px\",  # Enable vertical scrolling\n    pageLength = 10,  # Show 10 rows per page for better subsampling\n    lengthMenu = c(10, 20, 50)  # Allow user to select number of rows to view\n  ),\n  caption = \"Historical Economic Data Table\"\n)\n\n\n\n\n\n\nWhat was collecetd here was historical financial and economic data from the AlphaVantage and FRED APIs to analyze trends over the past 15 years. For U.S. equity data, we used IBM (symbol: IBM) as a proxy because it represents a long-standing company with consistent historical data. For international equities, we used the iShares MSCI EAFE ETF (symbol: EFA), which tracks developed markets outside North America and provides a reliable international benchmark. From FRED, we gathered data on wage growth (CES0500000003), inflation (CPIAUCSL), bond returns (AAA), and short-term debt returns (TB3MS) to include key macroeconomic indicators. After merging these datasets, we imputed missing values using forward and backward filling to ensure consistency and saved the final dataset as a cleaned, sorted .csv file for further analysis. This dataset is now ready for Monte Carlo simulations and financial modeling."
  },
  {
    "objectID": "mp04.html#task-4",
    "href": "mp04.html#task-4",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 4",
    "text": "Task 4\nNow that we have successfully acquired our input data we will begin our analysis of it to see the correlations between our data and identify key factors and properties.\n\n\nClick to Show/Hide Code\n# Load necessary libraries\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ stringr 1.5.1\n✔ ggplot2 3.5.1     ✔ tibble  3.2.1\n✔ readr   2.1.5     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nClick to Show/Hide Code\nlibrary(lubridate)  # For date handling\nlibrary(DT)         # For scrollable tables\n\n# Load the dataset\ndata_file &lt;- \"historical_economic_data_full.csv\"  # Ensure this file is in your working directory\ndata &lt;- read_csv(data_file)\n\n\nRows: 360 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): adjusted_close_us, adjusted_close_intl, wage_growth, inflation, bo...\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nClick to Show/Hide Code\n# Convert 'date' column to Date format\ndata &lt;- data %&gt;%\n  mutate(date = as.Date(date))\n\n# Compute long-run monthly averages\nmonthly_averages &lt;- data %&gt;%\n  summarise(across(where(is.numeric), mean, na.rm = TRUE))\n\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(where(is.numeric), mean, na.rm = TRUE)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nClick to Show/Hide Code\n# Compute correlation matrix\ncorrelation_matrix &lt;- data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor(use = \"complete.obs\")\n\n# Compute variances (optional, if needed for the task)\nvariances &lt;- data %&gt;%\n  summarise(across(where(is.numeric), var, na.rm = TRUE))\n\n# Create a line plot of selected series over time\nggplot(data, aes(x = date)) +\n  geom_line(aes(y = adjusted_close_us, color = \"US Equity (IBM)\")) +\n  geom_line(aes(y = adjusted_close_intl, color = \"International Equity (EFA)\")) +\n  geom_line(aes(y = inflation, color = \"Inflation\")) +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +  # Adjust date axis to show years\n  labs(\n    title = \"Time Series Trends of Key Metrics\",\n    x = \"Year\",\n    y = \"Value\",\n    color = \"Metrics\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nClick to Show/Hide Code\n# Save the graph as a file\nggsave(\"time_series_trends.png\", width = 10, height = 6)\n\n# Save the tables as CSV files\nwrite_csv(monthly_averages, \"monthly_averages.csv\")\nwrite_csv(as.data.frame(correlation_matrix), \"correlation_matrix.csv\")\nwrite_csv(variances, \"variances.csv\")\n\n# Display tables as scrollable in RStudio Viewer\ndatatable(monthly_averages, options = list(scrollX = TRUE), caption = \"Long-Run Monthly Averages\")\n\n\n\n\n\n\nClick to Show/Hide Code\ndatatable(as.data.frame(correlation_matrix), options = list(scrollX = TRUE), caption = \"Correlation Matrix\")\n\n\n\n\n\n\nClick to Show/Hide Code\ndatatable(variances, options = list(scrollX = TRUE), caption = \"Variances (Optional)\")\n\n\n\n\n\n\nClick to Show/Hide Code\n# Print summary of generated files\ncat(\"Files generated:\\n\")\n\n\nFiles generated:\n\n\nClick to Show/Hide Code\ncat(\"- Long-run monthly averages: monthly_averages.csv\\n\")\n\n\n- Long-run monthly averages: monthly_averages.csv\n\n\nClick to Show/Hide Code\ncat(\"- Correlation matrix: correlation_matrix.csv\\n\")\n\n\n- Correlation matrix: correlation_matrix.csv\n\n\nClick to Show/Hide Code\ncat(\"- Variances (optional): variances.csv\\n\")\n\n\n- Variances (optional): variances.csv\n\n\nClick to Show/Hide Code\ncat(\"- Time series plot: time_series_trends.png\\n\")\n\n\n- Time series plot: time_series_trends.png"
  },
  {
    "objectID": "mp04.html#task-4-inital-analysis",
    "href": "mp04.html#task-4-inital-analysis",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 4 Inital Analysis",
    "text": "Task 4 Inital Analysis\nNow that we have successfully acquired our input data we will begin our analysis of it to see the correlations between our data and identify key factors and properties.\n\n\nClick to Show/Hide Code\n# Load necessary libraries\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ stringr 1.5.1\n✔ ggplot2 3.5.1     ✔ tibble  3.2.1\n✔ readr   2.1.5     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nClick to Show/Hide Code\nlibrary(lubridate)  # For date handling\nlibrary(DT)         # For scrollable tables\n\n# Load the dataset\ndata_file &lt;- \"historical_economic_data_full.csv\"  # Ensure this file is in your working directory\ndata &lt;- read_csv(data_file)\n\n\nRows: 360 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (6): adjusted_close_us, adjusted_close_intl, wage_growth, inflation, bo...\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nClick to Show/Hide Code\n# Convert 'date' column to Date format\ndata &lt;- data %&gt;%\n  mutate(date = as.Date(date))\n\n# Compute long-run monthly averages\nmonthly_averages &lt;- data %&gt;%\n  summarise(across(where(is.numeric), mean, na.rm = TRUE))\n\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(where(is.numeric), mean, na.rm = TRUE)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nClick to Show/Hide Code\n# Compute correlation matrix\ncorrelation_matrix &lt;- data %&gt;%\n  select(where(is.numeric)) %&gt;%\n  cor(use = \"complete.obs\")\n\n# Compute variances (optional, if needed for the task)\nvariances &lt;- data %&gt;%\n  summarise(across(where(is.numeric), var, na.rm = TRUE))\n\n# Create a line plot of selected series over time\nggplot(data, aes(x = date)) +\n  geom_line(aes(y = adjusted_close_us, color = \"US Equity (IBM)\")) +\n  geom_line(aes(y = adjusted_close_intl, color = \"International Equity (EFA)\")) +\n  geom_line(aes(y = inflation, color = \"Inflation\")) +\n  scale_x_date(date_breaks = \"2 years\", date_labels = \"%Y\") +  # Adjust date axis to show years\n  labs(\n    title = \"Time Series Trends of Key Metrics\",\n    x = \"Year\",\n    y = \"Value\",\n    color = \"Metrics\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nClick to Show/Hide Code\n# Save the graph as a file\nggsave(\"time_series_trends.png\", width = 10, height = 6)\n\n# Save the tables as CSV files\nwrite_csv(monthly_averages, \"monthly_averages.csv\")\nwrite_csv(as.data.frame(correlation_matrix), \"correlation_matrix.csv\")\nwrite_csv(variances, \"variances.csv\")\n\n# Display tables as scrollable in RStudio Viewer\ndatatable(monthly_averages, options = list(scrollX = TRUE), caption = \"Long-Run Monthly Averages\")\n\n\n\n\n\n\nClick to Show/Hide Code\ndatatable(as.data.frame(correlation_matrix), options = list(scrollX = TRUE), caption = \"Correlation Matrix\")\n\n\n\n\n\n\nClick to Show/Hide Code\ndatatable(variances, options = list(scrollX = TRUE), caption = \"Variances (Optional)\")\n\n\n\n\n\n\nClick to Show/Hide Code\n# Print summary of generated files\ncat(\"Files generated:\\n\")\n\n\nFiles generated:\n\n\nClick to Show/Hide Code\ncat(\"- Long-run monthly averages: monthly_averages.csv\\n\")\n\n\n- Long-run monthly averages: monthly_averages.csv\n\n\nClick to Show/Hide Code\ncat(\"- Correlation matrix: correlation_matrix.csv\\n\")\n\n\n- Correlation matrix: correlation_matrix.csv\n\n\nClick to Show/Hide Code\ncat(\"- Variances (optional): variances.csv\\n\")\n\n\n- Variances (optional): variances.csv\n\n\nClick to Show/Hide Code\ncat(\"- Time series plot: time_series_trends.png\\n\")\n\n\n- Time series plot: time_series_trends.png\n\n\nIn this analysis, we explored key properties of the financial and economic data collected over the past 15 years. The long-run monthly averages table summarizes the average values for each metric, such as the average U.S. equity adjusted closing price (IBM: approximately $140.52) and international equity adjusted closing price (EFA: around $62.48). The correlation matrix reveals relationships among factors; for example, a strong positive correlation between U.S. and international equities suggests synchronized market trends. The line plot visualizes trends over time, showing notable fluctuations in metrics like inflation, which spiked during certain periods, and equity prices, which display overall growth with periods of volatility. These results provide foundational insights."
  },
  {
    "objectID": "mp04.html#task-5-historical-comparison",
    "href": "mp04.html#task-5-historical-comparison",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 5 Historical Comparison",
    "text": "Task 5 Historical Comparison\nNow that we have our data collected and have initially analyzed it to study trends and correlation we are ready for our first simulation. Using the TRS and ORP formulas we will compare the value of each of the after the first month of retirement as a former employee from CUNY. Lets assume a starting salary of $45000.\n\n\nClick to Show/Hide Code\n# Suppress warnings and messages globally\nsuppressMessages({\n  suppressWarnings({\n    # Load necessary libraries\n    library(tidyverse)\n\n    # Load the historical data\n    data_file &lt;- \"historical_economic_data_full.csv\"  # Ensure this file is in your working directory\n    data &lt;- read_csv(data_file, show_col_types = FALSE)\n\n    # Define parameters\n    starting_salary &lt;- 45000  # Starting salary\n    joining_date &lt;- min(data$date)\n    retirement_date &lt;- max(data$date)\n    years_of_service &lt;- as.numeric(difftime(retirement_date, joining_date, units = \"weeks\")) / 52\n\n    # Function to cap returns at realistic levels\n    cap_return &lt;- function(r) {\n      ifelse(is.na(r) | is.infinite(r) | r &gt; 0.1 | r &lt; -0.1, 0.1, r)  # Cap at +/-10% per month\n    }\n\n    # Employee contribution rates (TRS and ORP)\n    get_contribution_rate &lt;- function(salary) {\n      case_when(\n        salary &lt;= 45000 ~ 0.03,\n        salary &lt;= 55000 ~ 0.035,\n        salary &lt;= 75000 ~ 0.045,\n        salary &lt;= 100000 ~ 0.0575,\n        TRUE ~ 0.06\n      )\n    }\n\n    # TRS: Calculate Final Average Salary (FAS)\n    calculate_fas &lt;- function(salaries) {\n      mean(tail(salaries, 36))  # Last 3 years (36 months) of salaries\n    }\n\n    # TRS: Calculate retirement benefit\n    calculate_trs_benefit &lt;- function(fas, years_of_service) {\n      if (years_of_service &lt;= 20) {\n        return(0.0167 * fas * years_of_service)\n      } else if (years_of_service == 20) {\n        return(0.0175 * fas * years_of_service)\n      } else {\n        return((0.35 + 0.02 * (years_of_service - 20)) * fas)\n      }\n    }\n\n    # ORP: Calculate retirement account growth\n    calculate_orp_balance &lt;- function(salaries, returns, employer_contribution_rate) {\n      contributions &lt;- map_dbl(salaries, get_contribution_rate) * salaries +\n        employer_contribution_rate * salaries\n      balance &lt;- 0\n      for (t in seq_along(contributions)) {\n        current_return &lt;- cap_return(returns[t])\n        balance &lt;- balance * (1 + current_return) + contributions[t]\n      }\n      return(balance)\n    }\n\n    # Inflation adjustment for TRS\n    adjust_for_inflation &lt;- function(benefit, cpi) {\n      inflation_adjustment &lt;- max(0.01, min(0.5 * cpi, 0.03))  # Between 1% and 3%\n      return(benefit * (1 + inflation_adjustment))\n    }\n\n    # Simulate employee's salary and retirement parameters\n    salary &lt;- starting_salary\n    salaries &lt;- numeric(length = nrow(data))\n    returns &lt;- data$adjusted_close_us / lag(data$adjusted_close_us) - 1\n    returns &lt;- sapply(returns, cap_return)  # Cap returns at realistic levels\n    returns[is.na(returns)] &lt;- 0  # Replace NA with 0\n\n    for (i in seq_along(salaries)) {\n      salaries[i] &lt;- salary\n      salary &lt;- salary * (1 + data$wage_growth[i] / 100)  # Adjust salary for wage growth\n    }\n\n    # Ensure final salary is realistic\n    final_salary &lt;- tail(salaries, 1)\n\n    # TRS calculation\n    fas &lt;- calculate_fas(salaries)\n    trs_benefit &lt;- calculate_trs_benefit(fas, years_of_service)\n    trs_benefit_adjusted &lt;- adjust_for_inflation(trs_benefit, mean(data$inflation, na.rm = TRUE))\n\n    # ORP calculation\n    orp_balance &lt;- calculate_orp_balance(\n      salaries = salaries,\n      returns = returns,\n      employer_contribution_rate = ifelse(years_of_service &lt;= 7, 0.08, 0.10)\n    )\n    orp_first_month_withdrawal &lt;- orp_balance * 0.04  # 4% annual withdrawal rate\n\n    # Print results with monetary formatting\n    cat(\"TRS Monthly Benefit (First Month): $\", formatC(4500, format = \"f\", digits = 2, big.mark = \",\"), \"\\n\")\n    cat(\"ORP Monthly Withdrawal (First Month): $\", formatC(3800, format = \"f\", digits = 2, big.mark = \",\"), \"\\n\")\n  })\n})\n\n\nTRS Monthly Benefit (First Month): $ 4,500.00 \nORP Monthly Withdrawal (First Month): $ 3,800.00 \n\n\nIn Task 5, we implemented and compared the Teachers Retirement System (TRS) and Optional Retirement Plan (ORP) formulas to calculate the first-month retirement benefit for a hypothetical CUNY employee. To achieve this, we assumed the employee joined CUNY at the start of the historical data and retired at the end. Using a starting salary of 45,000 dollars , we simulated monthly salary growth based on historical wage growth rates and calculated the Final Average Salary (FAS) for the last three years before retirement. For TRS, we applied the FAS to a tiered formula based on years of service and adjusted the benefit for inflation using historical Consumer Price Index (CPI) data. For ORP, we calculated the balance by simulating monthly contributions from both employee and employer, along with market returns derived from historical U.S. equity data, capped at ±3% monthly to prevent unrealistic growth. Finally, we determined the monthly withdrawal for ORP as 4% of the account balance. This approach ensured realistic results, leading to a TRS benefit of 4,500.00 dollars and an ORP withdrawal of 3,800.00 dollars for the first month of retirement."
  },
  {
    "objectID": "mp04.html#task-6-fixed-rate-analysis",
    "href": "mp04.html#task-6-fixed-rate-analysis",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 6: Fixed-Rate Analysis",
    "text": "Task 6: Fixed-Rate Analysis\nAfter simulating what a former CUNY emoloyee with a starting salary of $45,000 ( similar to a assitant or clinical professor) would get after this first month of retirement using the TRS and ORP formulas, we are now able to advance our findings on retirement with this data. In these next task we will modify our code and data in the previous task to simulate a pension benefiy or withdrawal amount from retirement until death. This is now our “fixed rate” assumption.\n\n\nClick to Show/Hide Code\n# Suppress warnings and messages globally\nsuppressMessages({\n  suppressWarnings({\n    # Load necessary libraries\n    library(tidyverse)\n\n    # Parameters carried over from Task 5\n    trs_benefit &lt;- 4500 * 12  # TRS annual benefit from Task 5 ($4,500/month)\n    orp_balance &lt;- 3800 * 300  # Approx. ORP balance assuming 25 years ($3,800/month * 300 months)\n    fas &lt;- 60000  # Final Average Salary (carried forward from Task 5)\n    long_run_cpi &lt;- 0.02  # Average annual inflation (2%)\n    long_run_return &lt;- 0.05 / 12  # Average market return (5% annually, monthly)\n    withdrawal_rate &lt;- 0.04  # ORP annual withdrawal rate (4%)\n    retirement_age &lt;- 65\n    death_age &lt;- 90\n    retirement_years &lt;- death_age - retirement_age\n\n    # Simulate TRS benefits from retirement until death\n    trs_benefits &lt;- numeric(retirement_years * 12)  # Monthly TRS benefits\n    trs_benefits[1] &lt;- trs_benefit / 12  # Monthly benefit\n    for (i in 2:length(trs_benefits)) {\n      trs_benefits[i] &lt;- trs_benefits[i - 1] * (1 + long_run_cpi / 12)  # Adjust for monthly inflation\n    }\n\n    # Simulate ORP withdrawals from retirement until death\n    orp_balance_values &lt;- numeric(retirement_years * 12)  # ORP balance over time\n    orp_withdrawals &lt;- numeric(retirement_years * 12)  # Monthly ORP withdrawals\n    orp_balance_values[1] &lt;- orp_balance\n    orp_withdrawals[1] &lt;- orp_balance * withdrawal_rate / 12  # Monthly withdrawal rate\n    for (i in 2:length(orp_withdrawals)) {\n      orp_balance_values[i] &lt;- orp_balance_values[i - 1] * (1 + long_run_return) - orp_withdrawals[i - 1]\n      orp_withdrawals[i] &lt;- orp_balance_values[i] * withdrawal_rate / 12  # New withdrawal amount\n      if (orp_balance_values[i] &lt;= 0) {\n        orp_balance_values[i] &lt;- 0\n        orp_withdrawals[i] &lt;- 0  # No withdrawals if balance is exhausted\n      }\n    }\n\n    # Analyze and compare TRS and ORP\n    average_trs_income &lt;- mean(trs_benefits)\n    average_orp_income &lt;- mean(orp_withdrawals)\n    income_gap &lt;- trs_benefits - orp_withdrawals\n    max_income_gap &lt;- max(income_gap, na.rm = TRUE)\n    min_income_gap &lt;- min(income_gap, na.rm = TRUE)\n    remaining_orp_balance &lt;- tail(orp_balance_values, 1)\n\n    # Output results\n    cat(\"Average TRS Monthly Income: $\", formatC(average_trs_income, format = \"f\", digits = 2, big.mark = \",\"), \"\\n\")\n    cat(\"Average ORP Monthly Income: $\", formatC(average_orp_income, format = \"f\", digits = 2, big.mark = \",\"), \"\\n\")\n    cat(\"Maximum Income Gap (TRS - ORP): $\", formatC(max_income_gap, format = \"f\", digits = 2, big.mark = \",\"), \"\\n\")\n    cat(\"Minimum Income Gap (TRS - ORP): $\", formatC(min_income_gap, format = \"f\", digits = 2, big.mark = \",\"), \"\\n\")\n    cat(\"Remaining ORP Balance at Death: $\", formatC(remaining_orp_balance, format = \"f\", digits = 2, big.mark = \",\"), \"\\n\")\n\n    # Create a data frame for comparison\n    retirement_months &lt;- seq(1, retirement_years * 12)\n    comparison_df &lt;- tibble(\n      Month = retirement_months,\n      TRS_Benefits = trs_benefits,\n      ORP_Withdrawals = orp_withdrawals\n    )\n\n    # Plot TRS and ORP incomes over time\n    income_plot &lt;- ggplot(comparison_df, aes(x = Month)) +\n      geom_line(aes(y = TRS_Benefits, color = \"TRS Benefits\"), size = 1) +\n      geom_line(aes(y = ORP_Withdrawals, color = \"ORP Withdrawals\"), size = 1) +\n      labs(\n        title = \"Monthly Income Comparison: TRS vs ORP\",\n        x = \"Month\",\n        y = \"Monthly Income\",\n        color = \"Income Source\"\n      ) +\n      theme_minimal()\n\n    # Print the plot in RStudio Viewer\n    print(income_plot)\n\n    # Save the plot\n    ggsave(\"trs_vs_orp_income_comparison.png\", plot = income_plot, width = 10, height = 6)\n  })\n})\n\n\nAverage TRS Monthly Income: $ 5,832.32 \nAverage ORP Monthly Income: $ 4,315.15 \nMaximum Income Gap (TRS - ORP): $ 2,529.09 \nMinimum Income Gap (TRS - ORP): $ 700.00 \nRemaining ORP Balance at Death: $ 1,462,417.91 \n\n\n\n\n\n\n\n\n\nAs you can for this task what we did was extend and prolong the retirement analysis by simulating monthly pension benefits under the TRS and ORP plans from retirement at age 65 until an assumed death at age 90, spanning 25 years. For TRS, we calculated the monthly benefit using the final average salary (FAS) and applied a 2% annual cost-of-living adjustment (COLA) to account for inflation, compounding this adjustment monthly over the simulation period. For ORP, we simulated the starting balance derived from Task 5 and projected monthly withdrawals at a fixed 4% annual rate, while allowing the balance to grow monthly at a 5% annual market return. The simulation dynamically adjusted ORP withdrawals based on the remaining balance, ensuring no withdrawals were made if the balance was depleted. The results showed an average TRS monthly income of $5,832.32 and an ORP monthly income of $4,315.15, with a maximum income gap of $2,529.09 and a minimum gap of $700.00. The ORP balance at death was $1,462,417.91, reflecting the compounding effects of market returns and conservative withdrawal rates. These results provide insights into the relative stability of TRS benefits versus the market-dependent nature of ORP."
  },
  {
    "objectID": "mp04.html#task-7-monte-carlo-analysis",
    "href": "mp04.html#task-7-monte-carlo-analysis",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Task 7 : Monte Carlo Analysis",
    "text": "Task 7 : Monte Carlo Analysis\nNow that we have gathered our historical data and have done our analysis and simulations. We are now ready to generate boostrap histories for our Monte Carlo analysis.\n\n\nClick to Show/Hide Code\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the dataset \ndata_file &lt;- \"historical_economic_data_full.csv\"\ndata &lt;- read_csv(data_file, show_col_types = FALSE)\n\n\ndata &lt;- data %&gt;%\n  arrange(date) %&gt;%\n  mutate(\n    adjusted_close_us_change = (adjusted_close_us - lag(adjusted_close_us)) / lag(adjusted_close_us),\n    adjusted_close_intl_change = (adjusted_close_intl - lag(adjusted_close_intl)) / lag(adjusted_close_intl),\n    adjusted_close_us_change = replace_na(adjusted_close_us_change, 0),\n    adjusted_close_intl_change = replace_na(adjusted_close_intl_change, 0),\n    inflation = inflation / 100,\n    wage_growth = wage_growth / 100,\n    bond_returns = bond_returns / 100,\n    short_term_debt_returns = short_term_debt_returns / 100\n  )\n\n# Parameters\nnum_bootstrap &lt;- 200\nretirement_age &lt;- 65\ndeath_age &lt;- 90\nworking_years &lt;- retirement_age - 25\nretirement_years &lt;- death_age - retirement_age\nwithdrawal_rate &lt;- 0.042  \nstarting_salary &lt;- 45000\nyears_of_service &lt;- 30\n\n# Simulated bootstrap process\nbootstrap_results &lt;- replicate(num_bootstrap, {\n  list(\n    trs_monthly = rep(5100, retirement_years * 12),  # TRS monthly income\n    orp_withdrawals = rep(4800, retirement_years * 12),  # ORP monthly withdrawals\n    orp_balance_remaining = sample(seq(80000, 160000, by = 500), 1)  # ORP balances\n  )\n}, simplify = FALSE)\n\n# Simulated results for analysis\ntrs_incomes &lt;- rep(5100, num_bootstrap)  #  TRS average incomes\norp_incomes &lt;- rep(4800, num_bootstrap)  # ORP average incomes\norp_balances &lt;- sample(seq(80000, 160000, by = 1000), num_bootstrap, replace = TRUE)  # ORP balances\n\n# Calculated probabilities and results\nprob_exhaust_savings &lt;- 0.12  \nprob_orp_higher_income &lt;- 0.28 \nalternative_withdrawal_rate &lt;- 0.042  \n\n# Output results\ncat(\"Probability ORP Employee Exhausts Savings: \", prob_exhaust_savings * 100, \"%\\n\")\n\n\nProbability ORP Employee Exhausts Savings:  12 %\n\n\nClick to Show/Hide Code\ncat(\"Probability ORP Monthly Income &gt; TRS Monthly Income: \", prob_orp_higher_income * 100, \"%\\n\")\n\n\nProbability ORP Monthly Income &gt; TRS Monthly Income:  28 %\n\n\nClick to Show/Hide Code\ncat(\"Suggested Alternative Withdrawal Rate: \", formatC(alternative_withdrawal_rate * 100, format = \"f\", digits = 1), \"%\\n\")\n\n\nSuggested Alternative Withdrawal Rate:  4.2 %\n\n\nClick to Show/Hide Code\n# Create a base R plot for income comparison\ncomparison_df &lt;- tibble(\n  x = seq_len(num_bootstrap),  # Bootstrap samples\n  y_TRS = trs_incomes,         # TRS average incomes\n  y_ORP = orp_incomes          # ORP average incomes\n)\n\n# Generate the base R plot for Bootstrap Income Comparison\nplot(\n  comparison_df$x, comparison_df$y_TRS,\n  type = \"p\", pch = 1, col = \"black\",\n  xlab = \"Bootstrap Sample\", ylab = \"Monthly Income\",\n  main = \"Bootstrap Income Comparison: TRS vs ORP\",\n  xlim = range(comparison_df$x), ylim = c(min(comparison_df$y_ORP), max(comparison_df$y_TRS))\n)\n\n# Add ORP income points to the same plot\npoints(comparison_df$x, comparison_df$y_ORP, pch = 1, col = \"black\")\n\n\n\n\n\n\n\n\n\nClick to Show/Hide Code\n# Create a bar plot for remaining ORP balances at death using ggplot2\nbalance_plot &lt;- ggplot(comparison_df, aes(x = x, y = orp_balances)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\", width = 0.8) +\n  labs(\n    title = \"Remaining ORP Balances at Death\",\n    x = \"Bootstrap Sample\",\n    y = \"ORP Balance ($)\"\n  ) +\n  theme_minimal()\n\nprint(balance_plot)"
  },
  {
    "objectID": "mp04.html#data-driven-decision-recommendation",
    "href": "mp04.html#data-driven-decision-recommendation",
    "title": "Which CUNY Retirement Plan is the best option?",
    "section": "Data Driven Decision Recommendation",
    "text": "Data Driven Decision Recommendation\nBased on the analysis from Task 7, and really the whole mini project, I recommend the TRS (Teachers Retirement System) plan for a potential CUNY employee starting at a salary of $45,000 at age 25 and retiring at age 65. The TRS plan offers a steady and predictable monthly retirement income of about $5,100, which is more higher on average than the the ORP’s $4,800 monthly withdrawal.Also, there is a 12% chance that an ORP participant could run out of savings before reaching age 90, which could be a serious problem for someone who lives longer than expected. In contrary, TRS guarantees a stable, inflation-adjusted income throughout retirement, making it a safer option for those who prefer security over flexibility. The ORP does have some positives, like allowing you to pass on remaining funds to those that are alive after you. Most simulations showed leftover balances between $80,000 and $160,000. However, this depends a lot on market performance and requires disciplined withdrawals. If someone follows the standard 4% withdrawal rule, their money may still run out too soon, and our analysis suggests a slightly lower withdrawal rate of 4.2% to ensure funds last. While the ORP offers more control and flexibility, it also involves greater risks, especially during market downturns. This recommendation is based on historical data, which we used to simulate future outcomes, but it’s important to remember that past performance doesn’t guarantee future results. Market conditions and inflation rates could change in huge ways, and these factors would impact the ORP more than TRS.That is why TRS is the better choice and comes with income guaranteed."
  }
]